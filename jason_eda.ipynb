{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn as sk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/skessler/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_file_path = './data/federalist_papers_raw_gutenburg.txt'\n",
    "with open(raw_file_path, 'r') as f:\n",
    "    raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing individual papers into a dataframe\n",
    "\n",
    "The dataframe contains the paper number (e.g. FEDERALIST No. X) and the text body of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEDERALIST PAPERS \n",
      "FEDERALIST PAPERS\n",
      "\n",
      "FEDERALIST No. 1\n",
      "\n",
      "\n",
      "FEDERALIST No. 2\n",
      "\n",
      "\n",
      "FEDERALIST No. 3\n",
      "\n",
      "\n",
      "FEDERALIST No. 4\n",
      "\n",
      "\n",
      "FEDERALIST No. 5\n",
      "\n",
      "\n",
      "FEDERALIST No. 6\n",
      "\n",
      "\n",
      "FEDERALIST No. 7\n",
      "\n",
      "\n",
      "FEDERALIST No. 8\n",
      "\n",
      "\n",
      "FEDERALIST No. 9\n",
      "\n",
      "\n",
      "FEDERALIST No. 10\n",
      "\n",
      "FEDERALIST No. 11\n",
      "\n",
      "FEDERALIST No. 12\n",
      "\n",
      "FEDERALIST No. 13\n",
      "\n",
      "FEDERALIST No. 14\n",
      "\n",
      "FEDERALIST No. 15\n",
      "\n",
      "FEDERALIST No. 16\n",
      "\n",
      "FEDERALIST No. 17\n",
      "\n",
      "FEDERALIST No. 18\n",
      "\n",
      "FEDERALIST No. 19\n",
      "\n",
      "FEDERALIST No. 20\n",
      "\n",
      "FEDERALIST No. 21\n",
      "\n",
      "FEDERALIST No. 22\n",
      "\n",
      "FEDERALIST No. 23\n",
      "\n",
      "FEDERALIST No. 24\n",
      "\n",
      "FEDERALIST No. 25\n",
      "\n",
      "FEDERALIST No. 26\n",
      "\n",
      "FEDERALIST No. 27\n",
      "\n",
      "FEDERALIST No. 28\n",
      "\n",
      "FEDERALIST No. 29\n",
      "\n",
      "FEDERALIST No. 30\n",
      "\n",
      "FEDERALIST No. 31\n",
      "\n",
      "FEDERALIST No. 32\n",
      "\n",
      "FEDERALIST No. 33\n",
      "\n",
      "FEDERALIST No. 34\n",
      "\n",
      "FEDERALIST No. 35\n",
      "\n",
      "FEDERALIST No. 36\n",
      "\n",
      "FEDERALIST No. 37\n",
      "\n",
      "FEDERALIST No. 38\n",
      "\n",
      "FEDERALIST No. 39\n",
      "\n",
      "FEDERALIST No. 40\n",
      "\n",
      "FEDERALIST No. 41\n",
      "\n",
      "FEDERALIST No. 42\n",
      "\n",
      "FEDERALIST No. 43\n",
      "\n",
      "FEDERALIST No. 44\n",
      "\n",
      "FEDERALIST No. 45\n",
      "\n",
      "FEDERALIST No. 46\n",
      "\n",
      "FEDERALIST No. 47\n",
      "\n",
      "FEDERALIST No. 48\n",
      "\n",
      "FEDERALIST No. 49\n",
      "\n",
      "FEDERALIST No. 50\n",
      "\n",
      "FEDERALIST No. 51\n",
      "\n",
      "FEDERALIST No. 52\n",
      "\n",
      "FEDERALIST No. 53\n",
      "\n",
      "FEDERALIST No. 54\n",
      "\n",
      "FEDERALIST No. 55\n",
      "\n",
      "FEDERALIST No. 56\n",
      "\n",
      "FEDERALIST No. 57\n",
      "\n",
      "FEDERALIST No. 58\n",
      "\n",
      "FEDERALIST No. 59\n",
      "\n",
      "FEDERALIST No. 60\n",
      "\n",
      "FEDERALIST No. 61\n",
      "\n",
      "FEDERALIST No. 62\n",
      "\n",
      "FEDERALIST No. 63\n",
      "\n",
      "FEDERALIST No. 64\n",
      "\n",
      "FEDERALIST No. 65\n",
      "\n",
      "FEDERALIST No. 66\n",
      "\n",
      "FEDERALIST No. 67\n",
      "\n",
      "FEDERALIST No. 68\n",
      "\n",
      "FEDERALIST No. 69\n",
      "\n",
      "FEDERALIST No. 70\n",
      "\n",
      "FEDERALIST No. 71\n",
      "\n",
      "FEDERALIST No. 72\n",
      "\n",
      "FEDERALIST No. 73\n",
      "\n",
      "FEDERALIST No. 74\n",
      "\n",
      "FEDERALIST No. 75\n",
      "\n",
      "FEDERALIST No. 76\n",
      "\n",
      "FEDERALIST No. 77\n",
      "\n",
      "FEDERALIST No. 78\n",
      "\n",
      "FEDERALIST No. 79\n",
      "\n",
      "FEDERALIST No. 80\n",
      "\n",
      "FEDERALIST No. 81\n",
      "\n",
      "FEDERALIST No. 82\n",
      "\n",
      "FEDERALIST No. 83\n",
      "\n",
      "FEDERALIST No. 84\n",
      "\n",
      "FEDERALIST No. 85\n",
      "\n",
      "FEDERALIST PAPERS \n"
     ]
    }
   ],
   "source": [
    "# identifying potential paper starting indices\n",
    "indices = [word.start() for word in re.finditer('FEDERALIST', raw)]\n",
    "\n",
    "# But not all instances of 'FEDERALIST' are at the beginning of a paper.\n",
    "for i in indices:\n",
    "    print(raw[i:(i+18)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Parsing and creating dataframe\n",
    "data = pd.DataFrame(columns=['num','body'])\n",
    "for i in range(len(indices)): # iterate over potential paper beginnings\n",
    "    start = indices[i]\n",
    "    if i == len(indices) - 1:\n",
    "        end = None # used if this is the last element of indices\n",
    "    else:\n",
    "        end = indices[i+1]\n",
    "\n",
    "    full = raw[start:end] # extract full text corresponding to this instance of 'FEDERALIST'\n",
    "    \n",
    "    # Searching for string that is only found at very beggining of a paper\n",
    "    body_start = re.search('To the People of the State of New York', full)\n",
    "    if body_start:\n",
    "        # if found, then the paper starts immediately after\n",
    "        body_start = body_start.end() + 1\n",
    "    else:\n",
    "        # no body, so this isn't an instance of 'FEDERALIST' that begins a paper\n",
    "        # skip this iteration\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    body = full[body_start: ].strip() # extract the body from the full text\n",
    "    title = full[0:20] # extract the title from the full text\n",
    "    paper_num = re.findall(r'\\d+', title) # extract paper number from the title\n",
    "    paper_num = int(paper_num[0]) # converting to integer\n",
    "\n",
    "    # appending row to the dataframe\n",
    "    data = data.append({'num':paper_num,'body':body}, ignore_index=True)\n",
    "    \n",
    "# setting the paper numbers to be the index\n",
    "data.set_index('num', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AFTER an unequivocal experience of the ineffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>WHEN the people of America reflect that they a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>IT IS not a new observation that the people of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>MY LAST paper assigned several reasons why the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>QUEEN ANNE, in her letter of the 1st July, 170...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>THE three last numbers of this paper have been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>IT IS sometimes asked, with an air of seeming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>ASSUMING it therefore as an established truth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>A FIRM Union will be of the utmost moment to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>AMONG the numerous advantages promised by a we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>THE importance of the Union, in a commercial l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>THE effects of Union upon the commercial prosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>As CONNECTED with the subject of revenue, we m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>WE HAVE seen the necessity of the Union, as ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>IN THE course of the preceding papers, I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>THE tendency of the principle of legislation f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>AN OBJECTION, of a nature different from that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>AMONG the confederacies of antiquity, the most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>THE examples of ancient confederacies, cited i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>THE United Netherlands are a confederacy of re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.0</th>\n",
       "      <td>HAVING in the three last numbers taken a summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22.0</th>\n",
       "      <td>IN ADDITION to the defects already enumerated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23.0</th>\n",
       "      <td>THE necessity of a Constitution, at least equa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>TO THE powers proposed to be conferred upon th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25.0</th>\n",
       "      <td>IT MAY perhaps be urged that the objects enume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26.0</th>\n",
       "      <td>IT WAS a thing hardly to be expected that in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27.0</th>\n",
       "      <td>IT HAS been urged, in different shapes, that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <td>THAT there may happen cases in which the natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.0</th>\n",
       "      <td>THE power of regulating the militia, and of co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>IT HAS been already observed that the federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56.0</th>\n",
       "      <td>THE SECOND charge against the House of Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57.0</th>\n",
       "      <td>THE THIRD charge against the House of Represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58.0</th>\n",
       "      <td>THE remaining charge against the House of Repr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.0</th>\n",
       "      <td>THE natural order of the subject leads us to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60.0</th>\n",
       "      <td>WE HAVE seen, that an uncontrollable power ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61.0</th>\n",
       "      <td>THE more candid opposers of the provision resp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62.0</th>\n",
       "      <td>HAVING examined the constitution of the House ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63.0</th>\n",
       "      <td>A FIFTH desideratum, illustrating the utility ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64.0</th>\n",
       "      <td>IT IS a just and not a new observation, that e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65.0</th>\n",
       "      <td>THE remaining powers which the plan of the con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66.0</th>\n",
       "      <td>A REVIEW of the principal objections that have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67.0</th>\n",
       "      <td>THE constitution of the executive department o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68.0</th>\n",
       "      <td>THE mode of appointment of the Chief Magistrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.0</th>\n",
       "      <td>I PROCEED now to trace the real characters of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70.0</th>\n",
       "      <td>THERE is an idea, which is not without its adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71.0</th>\n",
       "      <td>DURATION in office has been mentioned as the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72.0</th>\n",
       "      <td>THE administration of government, in its large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73.0</th>\n",
       "      <td>THE third ingredient towards constituting the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74.0</th>\n",
       "      <td>THE President of the United States is to be \"c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75.0</th>\n",
       "      <td>THE President is to have power, \"by and with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76.0</th>\n",
       "      <td>THE President is \"to nominate, and, by and wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.0</th>\n",
       "      <td>IT HAS been mentioned as one of the advantages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78.0</th>\n",
       "      <td>WE PROCEED now to an examination of the judici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79.0</th>\n",
       "      <td>NEXT to permanency in office, nothing can cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80.0</th>\n",
       "      <td>TO JUDGE with accuracy of the proper extent of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81.0</th>\n",
       "      <td>LET US now return to the partition of the judi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82.0</th>\n",
       "      <td>THE erection of a new government, whatever car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83.0</th>\n",
       "      <td>THE objection to the plan of the convention, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84.0</th>\n",
       "      <td>IN THE course of the foregoing review of the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85.0</th>\n",
       "      <td>ACCORDING to the formal division of the subjec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body\n",
       "num                                                    \n",
       "1.0   AFTER an unequivocal experience of the ineffic...\n",
       "2.0   WHEN the people of America reflect that they a...\n",
       "3.0   IT IS not a new observation that the people of...\n",
       "4.0   MY LAST paper assigned several reasons why the...\n",
       "5.0   QUEEN ANNE, in her letter of the 1st July, 170...\n",
       "6.0   THE three last numbers of this paper have been...\n",
       "7.0   IT IS sometimes asked, with an air of seeming ...\n",
       "8.0   ASSUMING it therefore as an established truth ...\n",
       "9.0   A FIRM Union will be of the utmost moment to t...\n",
       "10.0  AMONG the numerous advantages promised by a we...\n",
       "11.0  THE importance of the Union, in a commercial l...\n",
       "12.0  THE effects of Union upon the commercial prosp...\n",
       "13.0  As CONNECTED with the subject of revenue, we m...\n",
       "14.0  WE HAVE seen the necessity of the Union, as ou...\n",
       "15.0  IN THE course of the preceding papers, I have ...\n",
       "16.0  THE tendency of the principle of legislation f...\n",
       "17.0  AN OBJECTION, of a nature different from that ...\n",
       "18.0  AMONG the confederacies of antiquity, the most...\n",
       "19.0  THE examples of ancient confederacies, cited i...\n",
       "20.0  THE United Netherlands are a confederacy of re...\n",
       "21.0  HAVING in the three last numbers taken a summa...\n",
       "22.0  IN ADDITION to the defects already enumerated ...\n",
       "23.0  THE necessity of a Constitution, at least equa...\n",
       "24.0  TO THE powers proposed to be conferred upon th...\n",
       "25.0  IT MAY perhaps be urged that the objects enume...\n",
       "26.0  IT WAS a thing hardly to be expected that in a...\n",
       "27.0  IT HAS been urged, in different shapes, that a...\n",
       "28.0  THAT there may happen cases in which the natio...\n",
       "29.0  THE power of regulating the militia, and of co...\n",
       "30.0  IT HAS been already observed that the federal ...\n",
       "...                                                 ...\n",
       "56.0  THE SECOND charge against the House of Represe...\n",
       "57.0  THE THIRD charge against the House of Represen...\n",
       "58.0  THE remaining charge against the House of Repr...\n",
       "59.0  THE natural order of the subject leads us to c...\n",
       "60.0  WE HAVE seen, that an uncontrollable power ove...\n",
       "61.0  THE more candid opposers of the provision resp...\n",
       "62.0  HAVING examined the constitution of the House ...\n",
       "63.0  A FIFTH desideratum, illustrating the utility ...\n",
       "64.0  IT IS a just and not a new observation, that e...\n",
       "65.0  THE remaining powers which the plan of the con...\n",
       "66.0  A REVIEW of the principal objections that have...\n",
       "67.0  THE constitution of the executive department o...\n",
       "68.0  THE mode of appointment of the Chief Magistrat...\n",
       "69.0  I PROCEED now to trace the real characters of ...\n",
       "70.0  THERE is an idea, which is not without its adv...\n",
       "71.0  DURATION in office has been mentioned as the s...\n",
       "72.0  THE administration of government, in its large...\n",
       "73.0  THE third ingredient towards constituting the ...\n",
       "74.0  THE President of the United States is to be \"c...\n",
       "75.0  THE President is to have power, \"by and with t...\n",
       "76.0  THE President is \"to nominate, and, by and wit...\n",
       "77.0  IT HAS been mentioned as one of the advantages...\n",
       "78.0  WE PROCEED now to an examination of the judici...\n",
       "79.0  NEXT to permanency in office, nothing can cont...\n",
       "80.0  TO JUDGE with accuracy of the proper extent of...\n",
       "81.0  LET US now return to the partition of the judi...\n",
       "82.0  THE erection of a new government, whatever car...\n",
       "83.0  THE objection to the plan of the convention, w...\n",
       "84.0  IN THE course of the foregoing review of the C...\n",
       "85.0  ACCORDING to the formal division of the subjec...\n",
       "\n",
       "[85 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_body(body):\n",
    "    '''\n",
    "    Function for cleaning the body of a federalist paper.\n",
    "    It just cleans up the whitespace right now, but we can add more\n",
    "    '''\n",
    "    body = re.sub(r'\\s+', ' ', body)\n",
    "    \n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['body'] = data['body'].apply(clean_text_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_file_path = './cleaned_papers_testing.csv'\n",
    "# data.to_csv(save_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv(save_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Borrowing liberally from two Kaggle NLP tutorials.\n",
    "\n",
    "Spooky Authorship: https://www.kaggle.com/cgump3rt/spooky-eda\n",
    "\n",
    "Movie Reviews: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labels and Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per Mosteller and Wallace (1963), 12 papers are disputed between Hamilton and Madison (49-58, 62, 63). Jay wrote just 5 (2-5, 64). Three were co-written by Hamilton and Madison (18-20), although the level of contribution from each is disputed. Madison wrote 14 and Hamilton wrote 43. Confusingly, this only adds to 77, which is the number they give for how many federalist papers there are. But there appears to actually be 85. Not sure where this discrepancy comes from. Wikipedia lists the papers as described in Douglass Adair's essay _The Disputed Federalist Papers_, with footnotes indicating which are disputed or joint. https://en.wikipedia.org/wiki/The_Federalist_Papers\n",
    "\n",
    "From this, we have Madison as the sole author for 14 (10, 14, 37-48) and Hamilton as the sole author for the remaining 51 (1, 6-9, 11-13, 15-17, 21-36, 59-61, 65-85).\n",
    "\n",
    "For labels, h=Hamilton, m=Madison, j=Jay, hm=Hamilton and Madison, d=disputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = np.array([2,3,4,5,64])\n",
    "m = np.array([10,14,37,38,39,40,41,42,43,44,45,46,47,48])\n",
    "hm = np.array([18,19,20])\n",
    "d = np.array([49,50,51,52,53,54,55,56,57,58,62,63])\n",
    "\n",
    "labels = np.array(['h']*85, dtype=object) # intially label all for Hamilton\n",
    "labels[j-1] = 'j'\n",
    "labels[m-1] = 'm'\n",
    "labels[hm-1] = 'hm'\n",
    "labels[d-1] = 'd'\n",
    "\n",
    "data['author'] = labels # adding author labels\n",
    "le = sk.preprocessing.LabelEncoder()\n",
    "le.fit(data['author'])\n",
    "data['label'] = le.transform(data['author']) # encoding labels as integers\n",
    "\n",
    "# Flagging papers with known authors. Treating dual-authorship as unknown.\n",
    "data['known'] = data['author'].apply(lambda author: author in ['j','h','m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adair believes that all of the joint and disputed texts were written by Madison, and that Hamilton had little if any input.\n",
    "adair_labels = labels\n",
    "adair_labels[np.concatenate([hm,d]) - 1] = 'm'\n",
    "\n",
    "data['adair'] = adair_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output all the papers as separate documents in their own folders so all future train data \n",
    "# can be stored and processed similarly. \n",
    "\n",
    "paper = 1\n",
    "for index, row in data.iterrows():\n",
    "    if row.loc['author'] == 'h':\n",
    "        location = 'train_data/alexander_hamilton'\n",
    "    elif row.loc['author'] == 'm':\n",
    "        location = 'train_data/james_madison'\n",
    "    elif row.loc['author'] == 'j':\n",
    "        location = 'train_data/john_jay'\n",
    "    else:\n",
    "        location = \"unknown_data\"\n",
    "        \n",
    "    file_name = 'federalist_paper_%d.txt' % paper\n",
    "    full_path = \"%s/%s\" % (location, file_name)\n",
    "    \n",
    "    with open(full_path, \"w\") as f:\n",
    "        f.write(row.loc['body'])\n",
    "    paper += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, training data is all known papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data[data.known].copy()\n",
    "test = data[~data.known].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring characteristics of entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author  label\n",
       "h       1        51\n",
       "j       3         5\n",
       "m       4        14\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying label counts.\n",
    "# should be 70 papers, since we have 12 disputed and 3 joint witheld from the full 85\n",
    "train.groupby('author').label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>3465</td>\n",
       "      <td>125577</td>\n",
       "      <td>667002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>220</td>\n",
       "      <td>9328</td>\n",
       "      <td>50036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>1176</td>\n",
       "      <td>43309</td>\n",
       "      <td>233306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_sentences  n_words  n_characters\n",
       "author                                    \n",
       "h              3465   125577        667002\n",
       "j               220     9328         50036\n",
       "m              1176    43309        233306"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of sentences, words, and characters in each paper\n",
    "train['n_sentences'] = train.body.apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "train['n_words'] = train.body.apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "train['n_characters'] = train.body.apply(lambda x: len(x))\n",
    "\n",
    "# Grouping counts by author\n",
    "train.groupby('author')[['n_sentences', 'n_words', 'n_characters']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned vocab size: 8095\n",
      "Shape of term-document matrix [n_samples, vocabulary_size]: (70, 8095)\n"
     ]
    }
   ],
   "source": [
    "# initialize count vectorizer\n",
    "cv = sk.feature_extraction.text.CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "# fitting bag of words model and learning the vocabulary\n",
    "train_features = cv.fit_transform(train.body)\n",
    "vocab = cv.get_feature_names()\n",
    "\n",
    "print('Learned vocab size:', len(vocab))\n",
    "print('Shape of term-document matrix [n_samples, vocabulary_size]:', train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>j</th>\n",
       "      <th>m</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>10351</td>\n",
       "      <td>516</td>\n",
       "      <td>3876</td>\n",
       "      <td>14743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>7230</td>\n",
       "      <td>359</td>\n",
       "      <td>2307</td>\n",
       "      <td>9896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>4547</td>\n",
       "      <td>288</td>\n",
       "      <td>1247</td>\n",
       "      <td>6082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2721</td>\n",
       "      <td>408</td>\n",
       "      <td>1164</td>\n",
       "      <td>4293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>2829</td>\n",
       "      <td>164</td>\n",
       "      <td>808</td>\n",
       "      <td>3801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>2300</td>\n",
       "      <td>160</td>\n",
       "      <td>754</td>\n",
       "      <td>3214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>1717</td>\n",
       "      <td>150</td>\n",
       "      <td>542</td>\n",
       "      <td>2409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1549</td>\n",
       "      <td>138</td>\n",
       "      <td>497</td>\n",
       "      <td>2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1329</td>\n",
       "      <td>57</td>\n",
       "      <td>481</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>1245</td>\n",
       "      <td>56</td>\n",
       "      <td>424</td>\n",
       "      <td>1725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>970</td>\n",
       "      <td>102</td>\n",
       "      <td>372</td>\n",
       "      <td>1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>856</td>\n",
       "      <td>82</td>\n",
       "      <td>452</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>923</td>\n",
       "      <td>38</td>\n",
       "      <td>249</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>925</td>\n",
       "      <td>68</td>\n",
       "      <td>159</td>\n",
       "      <td>1152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>785</td>\n",
       "      <td>53</td>\n",
       "      <td>265</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>751</td>\n",
       "      <td>90</td>\n",
       "      <td>232</td>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>716</td>\n",
       "      <td>57</td>\n",
       "      <td>259</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>703</td>\n",
       "      <td>65</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>702</td>\n",
       "      <td>73</td>\n",
       "      <td>247</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>578</td>\n",
       "      <td>83</td>\n",
       "      <td>234</td>\n",
       "      <td>895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>612</td>\n",
       "      <td>60</td>\n",
       "      <td>212</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>622</td>\n",
       "      <td>50</td>\n",
       "      <td>192</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>583</td>\n",
       "      <td>53</td>\n",
       "      <td>208</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>636</td>\n",
       "      <td>17</td>\n",
       "      <td>165</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>528</td>\n",
       "      <td>87</td>\n",
       "      <td>184</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>states</th>\n",
       "      <td>459</td>\n",
       "      <td>19</td>\n",
       "      <td>245</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>383</td>\n",
       "      <td>44</td>\n",
       "      <td>295</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>been</th>\n",
       "      <td>490</td>\n",
       "      <td>16</td>\n",
       "      <td>194</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>419</td>\n",
       "      <td>45</td>\n",
       "      <td>227</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>477</td>\n",
       "      <td>34</td>\n",
       "      <td>179</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disqualified</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closet</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disqualify</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closer</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rail</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intervals</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quest</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intrepid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quadruple</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intrenchments</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coincidence</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intolerant</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purview</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puts</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cognizances</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puzzle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puzzled</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quadrate</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimidation</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intervening</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimates</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coexistent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coexisted</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coerce</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coasts</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quarrel</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disproportioned</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queen</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queens</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fanciful</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8095 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     h    j     m  total\n",
       "the              10351  516  3876  14743\n",
       "of                7230  359  2307   9896\n",
       "to                4547  288  1247   6082\n",
       "and               2721  408  1164   4293\n",
       "in                2829  164   808   3801\n",
       "be                2300  160   754   3214\n",
       "that              1717  150   542   2409\n",
       "it                1549  138   497   2184\n",
       "is                1329   57   481   1867\n",
       "which             1245   56   424   1725\n",
       "as                 970  102   372   1444\n",
       "by                 856   82   452   1390\n",
       "this               923   38   249   1210\n",
       "would              925   68   159   1152\n",
       "have               785   53   265   1103\n",
       "or                 751   90   232   1073\n",
       "for                716   57   259   1032\n",
       "not                703   65   256   1024\n",
       "will               702   73   247   1022\n",
       "their              578   83   234    895\n",
       "with               612   60   212    884\n",
       "from               622   50   192    864\n",
       "are                583   53   208    844\n",
       "an                 636   17   165    818\n",
       "they               528   87   184    799\n",
       "states             459   19   245    723\n",
       "on                 383   44   295    722\n",
       "been               490   16   194    700\n",
       "government         419   45   227    691\n",
       "may                477   34   179    690\n",
       "...                ...  ...   ...    ...\n",
       "disqualified         1    0     0      1\n",
       "closet               0    0     1      1\n",
       "disqualify           1    0     0      1\n",
       "closer               1    0     0      1\n",
       "rail                 1    0     0      1\n",
       "intervals            1    0     0      1\n",
       "quest                1    0     0      1\n",
       "intrepid             0    0     1      1\n",
       "quadruple            1    0     0      1\n",
       "intrenchments        1    0     0      1\n",
       "coincidence          1    0     0      1\n",
       "intolerant           1    0     0      1\n",
       "purview              0    0     1      1\n",
       "puts                 1    0     0      1\n",
       "cognizances          1    0     0      1\n",
       "puzzle               0    0     1      1\n",
       "puzzled              0    0     1      1\n",
       "quadrate             1    0     0      1\n",
       "intimidation         1    0     0      1\n",
       "intervening          0    1     0      1\n",
       "intimates            1    0     0      1\n",
       "coexistent           0    0     1      1\n",
       "coexisted            1    0     0      1\n",
       "coerce               1    0     0      1\n",
       "coasts               1    0     0      1\n",
       "quarrel              0    1     0      1\n",
       "disproportioned      1    0     0      1\n",
       "queen                0    1     0      1\n",
       "queens               1    0     0      1\n",
       "fanciful             0    0     1      1\n",
       "\n",
       "[8095 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting distribution of word counts\n",
    "authors = train.author.unique()\n",
    "binarized = sk.preprocessing.label_binarize(train.author, authors) # binarizing for easier summing\n",
    "word_counts = binarized.T * train_features # matrix multiplication returns counts by word and author\n",
    "word_counts = word_counts.T # transposing to make columns authors\n",
    "\n",
    "# creating sorted dataframe with total word counts\n",
    "count_df = pd.DataFrame(word_counts, columns=authors, index=vocab)\n",
    "count_df['total'] = count_df.sum(axis=1) # summing author counts for each word to get total\n",
    "count_df.sort_values(by='total', inplace=True, ascending=False)\n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that there are obvious stop words that need to be removed. But first, we train a naive bayes with stop words included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m' 'h' 'h' 'h' 'm' 'm' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h']\n"
     ]
    }
   ],
   "source": [
    "test_features = cv.transform(test.body)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_features, train.label)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "predictions_1 = clf.predict(test_features)\n",
    "\n",
    "print(le.inverse_transform(predictions_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the above process, but this time using stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>j</th>\n",
       "      <th>m</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>states</th>\n",
       "      <td>459</td>\n",
       "      <td>19</td>\n",
       "      <td>245</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>419</td>\n",
       "      <td>45</td>\n",
       "      <td>227</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>435</td>\n",
       "      <td>15</td>\n",
       "      <td>155</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>power</th>\n",
       "      <td>400</td>\n",
       "      <td>19</td>\n",
       "      <td>138</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>constitution</th>\n",
       "      <td>240</td>\n",
       "      <td>6</td>\n",
       "      <td>143</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>221</td>\n",
       "      <td>43</td>\n",
       "      <td>122</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>union</th>\n",
       "      <td>242</td>\n",
       "      <td>22</td>\n",
       "      <td>75</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>244</td>\n",
       "      <td>31</td>\n",
       "      <td>53</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>167</td>\n",
       "      <td>15</td>\n",
       "      <td>70</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>federal</th>\n",
       "      <td>137</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>171</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public</th>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>powers</th>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>executive</th>\n",
       "      <td>129</td>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>general</th>\n",
       "      <td>139</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ought</th>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>146</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>united</th>\n",
       "      <td>145</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men</th>\n",
       "      <td>158</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>159</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>160</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>particular</th>\n",
       "      <td>124</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>different</th>\n",
       "      <td>113</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legislative</th>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laws</th>\n",
       "      <td>130</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>necessary</th>\n",
       "      <td>103</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convention</th>\n",
       "      <td>84</td>\n",
       "      <td>13</td>\n",
       "      <td>68</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plan</th>\n",
       "      <td>130</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legislature</th>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modest</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasonableness</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gorgons</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goods</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charms</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoners</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charm</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charter</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knot</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gravely</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asleep</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aside</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratifying</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grasping</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asiatic</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concealment</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grand</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ravages</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rave</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ravings</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rays</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ashamed</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reached</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concealments</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deeper</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7827 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  h   j    m  total\n",
       "states          459  19  245    723\n",
       "government      419  45  227    691\n",
       "state           435  15  155    605\n",
       "power           400  19  138    557\n",
       "constitution    240   6  143    389\n",
       "people          221  43  122    386\n",
       "union           242  22   75    339\n",
       "national        244  31   53    328\n",
       "authority       179   1   75    255\n",
       "great           167  15   70    252\n",
       "federal         137   4  105    246\n",
       "shall           171  10   56    237\n",
       "public          161   5   67    233\n",
       "powers           99   5  128    232\n",
       "executive       129   3   94    226\n",
       "general         139  11   73    223\n",
       "ought           159   2   59    220\n",
       "new             146   8   59    213\n",
       "united          145  10   54    209\n",
       "men             158  26   21    205\n",
       "time            159   9   33    201\n",
       "body            160   5   35    200\n",
       "particular      124   8   50    182\n",
       "different       113  13   48    174\n",
       "legislative      83   2   83    168\n",
       "laws            130  11   25    166\n",
       "necessary       103   7   55    165\n",
       "convention       84  13   68    165\n",
       "plan            130   6   26    162\n",
       "legislature     121   4   32    157\n",
       "...             ...  ..  ...    ...\n",
       "modest            1   0    0      1\n",
       "reasonableness    1   0    0      1\n",
       "gorgons           1   0    0      1\n",
       "goods             1   0    0      1\n",
       "charms            1   0    0      1\n",
       "reasoners         1   0    0      1\n",
       "charm             1   0    0      1\n",
       "charter           1   0    0      1\n",
       "knot              1   0    0      1\n",
       "reader            0   0    1      1\n",
       "asia              1   0    0      1\n",
       "gravely           1   0    0      1\n",
       "gratitude         1   0    0      1\n",
       "asleep            1   0    0      1\n",
       "aside             1   0    0      1\n",
       "gratifying        1   0    0      1\n",
       "grasping          0   0    1      1\n",
       "modify            1   0    0      1\n",
       "asiatic           1   0    0      1\n",
       "concealment       1   0    0      1\n",
       "grand             1   0    0      1\n",
       "ravages           1   0    0      1\n",
       "rave              1   0    0      1\n",
       "ravings           1   0    0      1\n",
       "raw               0   0    1      1\n",
       "rays              1   0    0      1\n",
       "ashamed           1   0    0      1\n",
       "reached           1   0    0      1\n",
       "concealments      1   0    0      1\n",
       "deeper            1   0    0      1\n",
       "\n",
       "[7827 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize count vectorizer\n",
    "cv = sk.feature_extraction.text.CountVectorizer(analyzer = \"word\", stop_words='english')\n",
    "\n",
    "# fitting bag of words model and learning the vocabulary\n",
    "train_features = cv.fit_transform(train.body)\n",
    "vocab = cv.get_feature_names()\n",
    "\n",
    "# Getting distribution of word counts\n",
    "authors = train.author.unique()\n",
    "binarized = sk.preprocessing.label_binarize(train.author, authors) # binarizing for easier summing\n",
    "word_counts = binarized.T * train_features # matrix multiplication returns counts by word and author\n",
    "word_counts = word_counts.T # transposing to make columns authors\n",
    "\n",
    "# creating sorted dataframe with total word counts\n",
    "count_df = pd.DataFrame(word_counts, columns=authors, index=vocab)\n",
    "count_df['total'] = count_df.sum(axis=1) # summing author counts for each word to get total\n",
    "count_df.sort_values(by='total', inplace=True, ascending=False)\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m' 'h' 'h' 'h' 'm' 'm' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h']\n"
     ]
    }
   ],
   "source": [
    "test_features = cv.transform(test.body)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_features, train.label)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "predictions_2 = clf.predict(test_features)\n",
    "\n",
    "print(le.inverse_transform(predictions_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying with tf-idf transform and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h' 'h']\n"
     ]
    }
   ],
   "source": [
    "# initialize count vectorizer\n",
    "cv = sk.feature_extraction.text.TfidfVectorizer(analyzer = \"word\", stop_words='english')\n",
    "\n",
    "# fitting bag of words model and learning the vocabulary\n",
    "train_features = cv.fit_transform(train.body)\n",
    "vocab = cv.get_feature_names()\n",
    "\n",
    "# Getting distribution of word counts\n",
    "authors = train.author.unique()\n",
    "binarized = sk.preprocessing.label_binarize(train.author, authors) # binarizing for easier summing\n",
    "word_counts = binarized.T * train_features # matrix multiplication returns counts by word and author\n",
    "word_counts = word_counts.T # transposing to make columns authors\n",
    "\n",
    "test_features = cv.transform(test.body)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_features, train.label)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "predictions_3 = clf.predict(test_features)\n",
    "\n",
    "print(le.inverse_transform(predictions_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51.0</th>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63.0</th>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1  2  3\n",
       "num          \n",
       "18.0  m  m  h\n",
       "19.0  h  h  h\n",
       "20.0  h  h  h\n",
       "49.0  h  h  h\n",
       "50.0  m  m  h\n",
       "51.0  m  m  h\n",
       "52.0  h  h  h\n",
       "53.0  h  h  h\n",
       "54.0  h  h  h\n",
       "55.0  h  h  h\n",
       "56.0  h  h  h\n",
       "57.0  h  h  h\n",
       "58.0  h  h  h\n",
       "62.0  h  h  h\n",
       "63.0  h  h  h"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame([le.inverse_transform(predictions_1),\n",
    "                        le.inverse_transform(predictions_2),\n",
    "                        le.inverse_transform(predictions_3)])\n",
    "results = results.transpose()\n",
    "results.columns = ['1', '2', '3']\n",
    "results.set_index(test.index, inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Adair believes all of the disputed works, including the three joint-authorship works (18-20), were written by Madison. Mosteller and Wallace's analysis also leaned heavily toward Madison with the exception of papers 55 and 20, where the evidence was less-heavily toward Madison or ambiguous, respectively.\n",
    "\n",
    "Unfortunately, these basic bag-of-words classifiers with Naive Bayes predict predominantly Hamilton. Clearly we have some work to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
