{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# RNNLM Model\n",
    "import rnnlm\n",
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary class holds the vocabulary and the mapping between words and ids for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "'''Vocabulary class, nearly identical to that used in a4'''\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to massage the cleaned data into indivdual words. Punctuation at the end of a word is split into its own distince word. Also accomplishes some minor data cleaning, removing nonsense characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    rep_dict = {'\\n':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xa0':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xc3':' '\n",
    "                ,'\\xa9':' '\n",
    "                ,'\\xef':' '\n",
    "                ,'\\xbb':' '\n",
    "                ,'\\xbf':' '\n",
    "                ,'\\xa6':' '\n",
    "                ,'\\xb9':' '\n",
    "                ,'\\xa3':' '\n",
    "                ,'\\xbd':' '\n",
    "                ,'\\xb4':' '\n",
    "                ,'\\xcb':' '\n",
    "                ,'\\x9a':' '\n",
    "                ,'\\x86':' '\n",
    "                ,'\\xcf':' '\n",
    "                ,'\\x84':' '\n",
    "                ,'\\xce':' '\n",
    "                ,'\\x87':' '\n",
    "                ,'\\xe2':' '\n",
    "                ,'\\x80':' '\n",
    "                ,'\\x94':' '\n",
    "               }\n",
    "    for word in replace_all(words, rep_dict).split(' '):   \n",
    "        if word:\n",
    "            if word[-1] in ('.', ',', '?', ';', '!'):\n",
    "                punk = word[-1]\n",
    "                current.append(punk)\n",
    "                word = word[0:-1]\n",
    "\n",
    "            word = canonicalize_word(word)\n",
    "            current.append(word)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_train_data reads in everything in train_data directory from which it generates the vocab (of type Vocabulary defined above), the author_to_id map (which maps author names to ids) and two arrays, one contains the text of each file (as a list of word ids) under \"train_data\". The other holds the author id for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        print author, author_id\n",
    "\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(np.array(current))\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    # replace words with ids\n",
    "    for i, x in enumerate(X):\n",
    "        # X[i] = np.array(x) # This line can be used to make sure your words are useful \n",
    "        X[i] = np.array(vocab.words_to_ids(x))\n",
    "\n",
    "    return vocab, np.array(X), np.array(y), author_to_id\n",
    "\n",
    "\n",
    "def id_to_author(author_to_id, id):\n",
    "    for author, author_id in author_to_id.iteritems():\n",
    "        if id == author_id:\n",
    "            return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the training data, note resulting number of classes (authors) and display some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thomas_paine 0\n",
      "thomas_jefferson 1\n",
      "john_adams 2\n",
      "james_madison 3\n",
      "alexander_hamilton 4\n",
      "james_monroe 5\n",
      "john_jay 6\n",
      "george_washington 7\n",
      "benjamin_franklin 8\n",
      "vocab.size 48471\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "vocab, X_train, y_train, author_to_id = load_train_data()\n",
    "num_classes = len(np.unique(y_train))\n",
    "print \"vocab.size\", vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the eval data in the same format as the training data. But each federalist paper here ends up in its own dictionary entry so that they can be scored/classified/attributed separately. Each is assumed to be written by James Madison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[[ 180    1 4299 ...,  398    5  500]]\n",
      "Who wrote Federalist paper 5 (John Jay should be answer): john_jay\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(vocab, eval_data_dir):\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        expanded_X = np.array(current)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = np.array([expanded_X])\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        if eval_data_dir == \"unknown_data\":\n",
    "            eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "        else:\n",
    "            if id == '5':\n",
    "                eval_y[id] = np.array([author_to_id['john_jay']])\n",
    "            elif id == '39':\n",
    "                eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "            else:\n",
    "                eval_y[id] = np.array([author_to_id['alexander_hamilton']])\n",
    "                \n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(vocab, \"unknown_data\")\n",
    "print eval_y['18']\n",
    "print eval_X['18']\n",
    "\n",
    "test_X, test_y = load_eval_data(vocab, \"test_data\")\n",
    "print \"Who wrote Federalist paper 5 (John Jay should be answer): %s\" % id_to_author(author_to_id, test_y['5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut up the publications based on batch_size, and max_time. To reduce how much code had to be changed from a4 this expands the author id for each document to be an author id for each word in the document. So if you had publication[1] = [1 2 3] and authors[1] = 1, this would output (assuming a batch of 1 and max time of 3) w = [1 2 3] and y = [1 1 1]. In the end we'll ignore all the loss for everything except the last word, but all the matrix functions and multiplications could work as is if I kept expanded the author to be associated with each word (since that is what the sequence math was doing, each word had a corresponding target word). Note: this also randomly shuffles the batches so that an given author's data is mixed through out the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'I', 'am'], ['mr', '.', 'anderson'], ['what', 'is', 'your']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_up_words(words, window_size=10, step_size=1):\n",
    "    clip_len = ((len(words)-1) / window_size) * window_size\n",
    "    words = words[:clip_len]\n",
    "    slices = []\n",
    "    num_words = len(words)\n",
    "    for index in range(0, num_words, step_size):\n",
    "        slices.append(words[index:index+window_size])\n",
    "    return slices\n",
    "        \n",
    "slice_up_words([\"hello\", \"I\", \"am\", \"mr\", \".\", \"anderson\", \"what\", \"is\", \"your\", \"name\", \"?\"], 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# maybe worth seeding here. or in the batch_generator function\n",
    "\n",
    "def shape_data_for_batching(publications, authors, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form.\"\"\"\n",
    "    all_w = None\n",
    "    all_y = []\n",
    "    for i, ids in enumerate(publications):\n",
    "        ids = np.array(slice_up_words(ids, max_time, max_time))\n",
    "        y = np.full_like(ids, authors[i])\n",
    "        \n",
    "        if all_w is not None:\n",
    "            all_w = np.append(all_w, ids, 0)\n",
    "            all_y = np.append(all_y, y, 0)\n",
    "            \n",
    "        else:\n",
    "            all_w = ids\n",
    "            all_y = y\n",
    "\n",
    "    # Yield batches in random order     \n",
    "    index = range(0, len(all_y)-1)\n",
    "    random.shuffle(index)   \n",
    "\n",
    "    all_w = [all_w[i] for i in index]\n",
    "    all_y = [all_y[i] for i in index]\n",
    "\n",
    "    return all_w, all_y\n",
    "\n",
    "def batch_generator(X_shaped, y_shaped, batch_size):\n",
    "    clip_len = ((len(X_shaped)-1) / batch_size) * batch_size\n",
    "    X_shaped = X_shaped[:clip_len]\n",
    "    y_shaped = y_shaped[:clip_len]  \n",
    "    for j in xrange(0, len(X_shaped), batch_size):\n",
    "        this_x = X_shaped[j:j+batch_size]\n",
    "        this_y = y_shaped[j:j+batch_size]\n",
    "        yield np.array(this_x), np.array(this_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "X_shaped, y_shaped = shape_data_for_batching(X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [3 3 3 3 3]\n",
      " [6 6 6 6 6]\n",
      " [1 1 1 1 1]\n",
      " [3 3 3 3 3]\n",
      " [1 1 1 1 1]\n",
      " [4 4 4 4 4]\n",
      " [1 1 1 1 1]\n",
      " [4 4 4 4 4]]\n",
      "[[   96   842    26     1    97]\n",
      " [    1 13710  1809     2 11606]\n",
      " [    1   378     3 20161 10479]\n",
      " [   28     5  3535    65     6]\n",
      " [  127     8  3050  3717     3]\n",
      " [ 1781     5  1378   113   139]\n",
      " [  132    17     1   552     3]\n",
      " [ 2134     3  4582   112   733]\n",
      " [  915    16     8  7779     6]\n",
      " [    7  1848    19    70  8850]]\n"
     ]
    }
   ],
   "source": [
    "for i, (w, y) in enumerate(batch_generator(X_shaped, y_shaped, 10)):\n",
    "    print y\n",
    "    print w \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the epoch. Mostly the same as a4, but there is no test phase (instead there is no a separate prediction phase) that didn't makes as much sense to me to have in this function, so it has its own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y,\n",
    "                     lm.initial_h_: h,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout}\n",
    "        \n",
    "        _, h, cost = session.run([train_op, lm.final_h_, lm.loss_], feed_dict)  \n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, authors, max_time, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up. Same as a4\n",
    "    bi = batch_generator(ids, authors, batch_size=100, max_time=max_time)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to predict a batch iterators data. Used post training to test the unknown federalist papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Summary:\n",
      "thomas_jefferson: 0.50\n",
      "john_adams: 0.50\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.5, 2: 0.5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_prediction_results(predictions, author_to_id, print_results=True):\n",
    "    '''\n",
    "    Takes the predictions for a set of text batches and calculates the percentage\n",
    "    of patches predicted for each author.\n",
    "    '''\n",
    "    counts = defaultdict(float)\n",
    "    for p in predictions:\n",
    "        counts[p] += 1\n",
    "\n",
    "    # getting prediction percentages to return for later viewing\n",
    "    predictions_num = len(predictions)\n",
    "    results_dict = {author_id:float(count)/predictions_num for author_id, count in counts.iteritems()}\n",
    "    \n",
    "    if print_results:\n",
    "        # prints results if indicated\n",
    "        print \"Prediction Summary:\"\n",
    "        for author_id, prediction in results_dict.iteritems():\n",
    "            print \"%s: %.2f\" % (id_to_author(author_to_id, author_id), prediction)\n",
    "        print \"\"\n",
    "\n",
    "    return results_dict\n",
    "    \n",
    "def predict_paper(lm, session, batch_iterator, authors, paper_name, print_results=True):\n",
    "    '''\n",
    "    Splits given paper into batches and predicts the author for each batch.\n",
    "    Passes these predictions to get_prediction_results() to tally the batches for each author\n",
    "    and arrive at a final prediction percentage.\n",
    "    '''\n",
    "    total_predictions = np.array([])\n",
    "    if print_results:\n",
    "        print \"Predicting for %s\" % paper_name\n",
    "    for i, (w, y) in enumerate(batch_iterator):        \n",
    "        if i == 0 and print_results:\n",
    "            print \"Truth:\", id_to_author(authors, y[0][0])\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y}\n",
    "        \n",
    "        cost, truths, logits, predictions = session.run([lm.loss_, lm.target_y_last_, lm.logits_last_, lm.predictions_], feed_dict)  \n",
    "        total_predictions = np.append(total_predictions, predictions.reshape(-1))\n",
    "\n",
    "    # gets final predictions, by author, for the given paper\n",
    "    results_dict = get_prediction_results(total_predictions, authors, print_results=print_results)\n",
    "    return results_dict\n",
    "\n",
    "def test_papers(lm, session, papers, labels, authors, batch_size, print_results=True):\n",
    "    '''\n",
    "    Predicts authorship for given papers.\n",
    "    Returns a dictionary of dictionaries containing the predictions by author for each paper.\n",
    "    '''\n",
    "    full_results = dict()\n",
    "    for key in papers:\n",
    "        prediction_bi = batch_generator(papers[key], labels[key], batch_size)\n",
    "        paper_results = predict_paper(lm, session, prediction_bi, authors, \"Federalist Paper %s\" % key, print_results=print_results)\n",
    "        full_results[key] = paper_results\n",
    "    \n",
    "    return full_results\n",
    "\n",
    "def create_predictions_dataframe(epoch_predictions):\n",
    "    '''\n",
    "    Input: List of predictions for each epoch, where an epochs predictions is a dictionary of dictionaries containing\n",
    "    the predictions for each paper by author.\n",
    "    Output: Pandas dataframe of results for each author by paper, with a flag indicating the epoch.\n",
    "    '''\n",
    "    df_list = []\n",
    "    for epoch in range(len(epoch_predictions)):\n",
    "        epoch_num = epoch + 1 # we indexed our epochs from 1, but this list is 0-indexed of course\n",
    "        df = pd.DataFrame(epoch_predictions[epoch])\n",
    "        df['epoch'] = epoch_num\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list)\n",
    "    \n",
    "get_prediction_results([1, 2, 1, 2], author_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 15\n",
    "batch_size = 40\n",
    "learning_rate = 0.1\n",
    "num_epochs =3\n",
    "\n",
    "save_predictions = True # flag to determine if we save the predictions on unkown papers in each epoch\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    num_classes=num_classes,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")\n",
    "\n",
    "\n",
    "X_train_shaped, y_train_shaped = shape_data_for_batching(X_train, y_train, max_time)\n",
    "\n",
    "X_test_shaped = {}\n",
    "y_test_shaped = {}\n",
    "for key in test_X:\n",
    "    X_test_shaped[key], y_test_shaped[key] = shape_data_for_batching(test_X[key], test_y[key], max_time)\n",
    "\n",
    "X_eval_shaped = {}\n",
    "y_eval_shaped = {}\n",
    "for key in eval_X:\n",
    "    X_eval_shaped[key], y_eval_shaped[key] = shape_data_for_batching(eval_X[key], eval_y[key], max_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same as a4, but instead of scoring the data we look at prediction results after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 4720]: seen 2832600 words at 28323 wps, loss = 1.395\n",
      "[epoch 1] Completed in 0:02:16\n",
      "[epoch 1] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.41\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.40\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.39\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.43\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.71\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.21\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 5633]: seen 3380400 words at 33802 wps, loss = 1.316\n",
      "[epoch 2] Completed in 0:01:53\n",
      "[epoch 2] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.27\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.65\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.31\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.66\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.60\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.38\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 5793]: seen 3476400 words at 34758 wps, loss = 1.279\n",
      "[epoch 3] Completed in 0:01:50\n",
      "[epoch 3] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.31\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.59\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.63\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.60\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.38\n",
      "\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.59\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.35\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.38\n",
      "james_madison: 0.14\n",
      "alexander_hamilton: 0.48\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.46\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.42\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.38\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.43\n",
      "james_madison: 0.11\n",
      "alexander_hamilton: 0.46\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.41\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.49\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.52\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.56\n",
      "james_madison: 0.09\n",
      "alexander_hamilton: 0.35\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.59\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.44\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.57\n",
      "james_madison: 0.05\n",
      "alexander_hamilton: 0.38\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.38\n",
      "james_madison: 0.14\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.43\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.71\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildClassifierGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    # List for saving the unkown paper predictions from each epoch\n",
    "    if save_predictions:\n",
    "        epoch_predictions = []\n",
    "        \n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(X_train_shaped, y_train_shaped, batch_size)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        # Run a training epoch.\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=100, learning_rate=learning_rate)\n",
    "    \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, eval_X['18'], eval_y['18'], max_time, name=\"Federalist Paper 18\")\n",
    "        #score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "\n",
    "        # test three of the federalist papers whose author is known after each epoch\n",
    "        test_predictions = test_papers(lm, session, X_test_shaped, y_test_shaped, author_to_id, batch_size, print_results=True)\n",
    "        \n",
    "        if save_predictions:\n",
    "            # testing against all unkown federalist papers\n",
    "            unk_predictions = test_papers(lm, session, X_eval_shaped, y_eval_shaped, author_to_id, batch_size, print_results=False)\n",
    "            epoch_predictions.append(unk_predictions)\n",
    "\n",
    "    # Testing final model against all unkown federalist papers\n",
    "    final_test = test_papers(lm, session, X_eval_shaped, y_eval_shaped, author_to_id, batch_size, print_results=True)    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting epoch predictions into a dataframe for analysis\n",
    "if save_predictions:\n",
    "    df = create_predictions_dataframe(epoch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0000</th>\n",
       "      <td>0.6167</td>\n",
       "      <td>0.6167</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.3583</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.4333</td>\n",
       "      <td>0.4833</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0000</th>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0000</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.2833</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.4333</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4417</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.4417</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0000</th>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.3563</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0000</th>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.0917</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.1167</td>\n",
       "      <td>0.1167</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0000</th>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.6083</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.6417</td>\n",
       "      <td>0.5083</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0000</th>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4417</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.4333</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3812</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0000</th>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.1437</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0000</th>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4875</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.4833</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           18     19     20     49     50     51     52     53     54     55  \\\n",
       "1.0000 0.6167 0.6167 0.6500 0.4000 0.5875 0.3583 0.4750 0.4333 0.4833 0.4667   \n",
       "3.0000 0.0833 0.1000 0.0875 0.1500 0.0750 0.2083 0.0750 0.1250 0.1500 0.1667   \n",
       "4.0000 0.3000 0.2833 0.2625 0.4500 0.3375 0.4333 0.4500 0.4417 0.3667 0.3667   \n",
       "1.0000 0.7000 0.5250 0.5625 0.2750 0.5250 0.2667 0.3750 0.3000 0.3500 0.4500   \n",
       "3.0000 0.0667 0.0500 0.0375 0.1125 0.0375 0.1250 0.0583 0.0583 0.1417 0.0917   \n",
       "4.0000 0.2333 0.4250 0.4000 0.6125 0.4375 0.6083 0.5667 0.6417 0.5083 0.4583   \n",
       "1.0000 0.7083 0.5750 0.5875 0.3000 0.5625 0.3333 0.4417 0.3250 0.3833 0.4583   \n",
       "3.0000 0.0667 0.0500 0.0625 0.1250 0.0875 0.1500 0.0833 0.0833 0.1667 0.1250   \n",
       "4.0000 0.2250 0.3750 0.3500 0.5750 0.3500 0.5167 0.4750 0.5917 0.4500 0.4167   \n",
       "\n",
       "           56     57     58     62     63  epoch  \n",
       "1.0000 0.4750 0.4083 0.4083 0.4500 0.5400      1  \n",
       "3.0000 0.1375 0.2250 0.1500 0.1375 0.1500      1  \n",
       "4.0000 0.3875 0.3667 0.4417 0.4125 0.3100      1  \n",
       "1.0000 0.3750 0.3833 0.3667 0.3563 0.4100      2  \n",
       "3.0000 0.0875 0.1167 0.1167 0.1000 0.0750      2  \n",
       "4.0000 0.5375 0.5000 0.5167 0.5437 0.5150      2  \n",
       "1.0000 0.4125 0.4333 0.3750 0.3812 0.4350      3  \n",
       "3.0000 0.1000 0.1083 0.1417 0.1437 0.1000      3  \n",
       "4.0000 0.4875 0.4583 0.4833 0.4750 0.4650      3  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "print author_to_id['james_madison']\n",
    "print author_to_id['george_washington']\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
