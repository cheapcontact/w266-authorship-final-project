{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# RNNLM Model\n",
    "import rnnlm\n",
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    for word in words.split(\" \"):   \n",
    "        if word[-1] in (\".\", ',', '?', ';', '!'):\n",
    "            punk = word[-1]\n",
    "            current.append(punk)\n",
    "            word = word[0:-1]\n",
    "\n",
    "        word = canonicalize_word(word)\n",
    "        current.append(word)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(np.array(current))\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    # replace words with ids\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = np.array(vocab.words_to_ids(x))\n",
    "\n",
    "    return vocab, np.array(X), np.array(y), author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33323\n",
      "{'thomas_jefferson': 0, 'john_adams': 1, 'alexander_hamilton': 3, 'george_washington': 6, 'james_madison': 2, 'james_monroe': 4, 'john_jay': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab, X_train, y_train, author_to_id = load_train_data()\n",
    "num_classes = len(np.unique(y_train))\n",
    "print vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[[ 179    1 1677 ...,  253    6  469]]\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(vocab):\n",
    "    eval_data_dir = \"unknown_data\"\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        expanded_X = np.array(current)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = np.array([expanded_X])\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "\n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(vocab)\n",
    "print eval_y['18']\n",
    "print eval_X['18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 5 5 5 5 5 6 6]\n",
      "[array([   11,    12,     8, ...,     6, 16924,  1444])\n",
      " array([ 1444,     4,  1432, ...,     3,     1, 17362])\n",
      " array([   48,    36,    16, ..., 27132,     6,  3617])\n",
      " array([   1,  155,  587, ...,    6,  116, 1274])\n",
      " array([  11,   36,  628, ...,    6,  488, 1274])\n",
      " array([  1, 425, 222, ...,   3,   6, 509])\n",
      " array([ 196,  628,   10, ...,    6,  145, 1274])\n",
      " array([ 196, 6450,    1, ...,    6,  587, 1274])\n",
      " array([   1,  425, 1031, ...,    6,  739, 1274])\n",
      " array([ 179,    1,  610, ...,    6, 3140, 1274])\n",
      " array([7863,    1,  132, ...,    6,   45, 1274])\n",
      " array([  39,   17,  608, ...,    6,  951, 1274])\n",
      " array([   1,   96,  342, ...,  638, 2642, 1274])\n",
      " array([   7, 6268,    1, ...,  117,    6, 4120])\n",
      " array([  11,   12,   20, ...,    6, 1387, 1274])\n",
      " array([   8, 1827, 1031, ...,    6, 1656, 1274])\n",
      " array([  38,    1,  353, ...,   12,    6, 1665])\n",
      " array([   1, 1777, 1031, ...,    6,   43, 1274])\n",
      " array([  11,   33,  363, ...,    6, 2060, 1274])\n",
      " array([   1,  176,   12, ...,    6,  525, 1274])\n",
      " array([11193,  6356,  3381, ...,  1817,     4, 25722])\n",
      " array([ 217,   30, 2480, ...,  117,    6,   96])\n",
      " array([ 2381,     4, 13666, ...,   481,     3, 25106])\n",
      " array([  11,   12,  784, ..., 2429,    6, 7841])\n",
      " array([   1,  544, 2537, ...,   18,    6,  190])\n",
      " array([  39,   17,  608, ...,  157,    6, 8592])\n",
      " array([ 595,    4, 3370, ...,  133,    6,  372])\n",
      " array([   1,  176,    3, ...,    6, 2174, 1274])\n",
      " array([   1,  384,    3, ...,    6, 1393, 1274])\n",
      " array([ 2008,     2,   372, ..., 22473,  2381,  8361])\n",
      " array([285,  69, 141, ..., 595,   6, 587])\n",
      " array([    6,    47, 17873, ...,     3,  4715, 26202])\n",
      " array([12491,     4, 26534, ...,    29, 18984,  8361])\n",
      " array([   1, 2282,    3, ...,    6,  145, 1274])\n",
      " array([   8, 2079,    3, ...,    3,    6, 2467])\n",
      " array([    2,    39,     1, ...,    46,     3, 19896])\n",
      " array([39, 17,  2, ..., 18,  6, 56])\n",
      " array([  11,   34,   29, ...,  157,    6, 8592])\n",
      " array([   1, 1120,  113, ...,    6, 1230, 1274])\n",
      " array([   1,  282,    3, ...,    6,  959, 1274])\n",
      " array([   1, 3682,    3, ..., 2741,    6,  140])\n",
      " array([  920,     3,     1, ...,   351,     3, 32840])\n",
      " array([  11,   36,    8, ...,    6, 4599, 1274])\n",
      " array([  696,     5, 31237, ...,  1039,     5, 26127])\n",
      " array([1792,   48,  429, ...,    6,  277, 1274])\n",
      " array([    6,    47, 26560, ...,  2257,     8, 25247])\n",
      " array([ 5163,     3,     1, ..., 25821, 23151, 24897])\n",
      " array([  805,     3,  6205, ..., 16229, 14636, 22083])\n",
      " array([    4,     1,  3925, ...,    83,   878, 21862])\n",
      " array([ 170,   39,  643, ...,    6,  218, 1274])\n",
      " array([  4,   1, 113, ...,  18,   6, 207])\n",
      " array([25773, 26343,   483, ...,    12,    20, 21773])\n",
      " array([  11,   34,   29, ...,  747,    6, 5875])\n",
      " array([  317,     4,     1, ..., 10941, 12050,     1])\n",
      " array([   1,  327,  264, ..., 1485,    6,  133])\n",
      " array([30933,     2, 13683, ...,    16,    28, 18943])\n",
      " array([  48, 2134,  724, ...,    6,  275, 1274])\n",
      " array([   8, 1479,  145, ..., 3909,    6,   48])\n",
      " array([   33,    11,  1707, ...,    26,     9, 19386])\n",
      " array([    1,   754,   140, ..., 25373,  6381,  8361])\n",
      " array([5068,   11,  147, ...,   18,    6,  132])\n",
      " array([    1,  4617, 25124, ...,    22,     8, 14709])\n",
      " array([   1,  330,    4, ...,    5,    6, 5569])\n",
      " array([   1,   60,    3, ...,    6, 2592, 1274])\n",
      " array([  11,   34,   29, ...,    6,   11, 1274])\n",
      " array([  39,  643,  141, ..., 2470,    6, 2424])\n",
      " array([    1,   176,    12, ...,     7, 12823,  1274])\n",
      " array([  188, 25490,     2, ...,   820,    12, 29803])\n",
      " array([    1,   325,   155, ...,  2845,     6, 26612])\n",
      " array([    1,   620,     3, ...,     6, 12065,  1274])\n",
      " array([  30,    2,  330, ...,    6,  842, 1274])\n",
      " array([  195,   121,  2396, ...,     3,     1, 16597])\n",
      " array([  1,  96,   3, ..., 476,   6, 140])\n",
      " array([   7, 8530,    3, ...,    6,   40, 1274])\n",
      " array([ 196,    7,    1, ...,    6,  288, 1274])\n",
      " array([    1,   394,     3, ..., 17861,  9521, 27125])\n",
      " array([    6,   333, 21820, ...,   972,    12, 29255])\n",
      " array([28693,     2,   372, ...,   518,     3, 15832])\n",
      " array([ 2381,     4,  6094, ...,     6,   770, 24507])\n",
      " array([   7,    1,  143, ...,   16,    1, 7052])\n",
      " array([  15,  880,   21, ...,    6,   84, 1274])\n",
      " array([  49,   12,   30, ...,  157,    6, 8592])\n",
      " array([    6,    47, 17874, ...,     5,  2723, 18365])\n",
      " array([  10,   49,   33, ..., 1461,    6, 1144])\n",
      " array([   1,  671,    3, ...,  619,    6, 1081])\n",
      " array([32277,   140,  7213, ...,  4774,     8, 22967])\n",
      " array([   4,  486,   21, ...,    6,  392, 1274])\n",
      " array([   47, 19503,     1, ...,  3284, 17581, 21068])\n",
      " array([  7, 874,   4, ...,   8,   6, 649])\n",
      " array([    1,    94,     3, ...,  3190,     1, 15094])\n",
      " array([19126,     2,   372, ...,     3,  7017, 25815])\n",
      " array([23441,     2,   140, ...,    30,  2209, 18529])\n",
      " array([922,   7, 319, ...,   1,   6,  94])\n",
      " array([   1,   50, 1640, ...,    6, 8698, 1274])\n",
      " array([    1,   438,     3, ..., 28364,     6, 21186])\n",
      " array([    4,     1,  1511, ...,     1,    43, 31050])\n",
      " array([ 48, 643, 141, ...,  87,   6, 765])\n",
      " array([   7,    1,  143, ...,    5,    6, 9357])\n",
      " array([   38,    47,   353, ...,     3,     6, 19844])\n",
      " array([ 5892,     2, 32974, ...,     6,   103,  1274])\n",
      " array([  100,     1,    94, ...,   158, 24078,  1274])\n",
      " array([  11,   12,   20, ...,   71,  119, 1274])\n",
      " array([ 158,  155,  587, ...,    6,  225, 1274])\n",
      " array([  11,   12,    8, ...,    6, 2467, 1274])\n",
      " array([   48,  1481,    21, ...,     6, 25701,  1432])\n",
      " array([ 1997,     2,   142, ..., 27139,  1432,  8361])]\n"
     ]
    }
   ],
   "source": [
    "print y_train\n",
    "print X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(publications, authors, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form.\"\"\"\n",
    "    for i, ids in enumerate(publications):\n",
    "        # Clip to multiple of max_time for convenience\n",
    "        clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
    "        \n",
    "        input_w = ids[:clip_len]     # current word\n",
    "        target_y = ids[1:clip_len+1]  # next word\n",
    "        # Reshape so we can select columns\n",
    "        input_w = input_w.reshape([batch_size,-1])\n",
    "\n",
    "        # Yield batches\n",
    "        for j in xrange(0, input_w.shape[1], max_time):\n",
    "            this_w = input_w[:,j:j+max_time]\n",
    "            yield this_w, np.full_like(this_w, authors[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[   11    12     8   539     3]\n",
      " [    4  3248     4 15631    31]\n",
      " [   28  1208    38 31402     3]\n",
      " [32160   298   187  1972    10]\n",
      " [  582    52   919 13039    13]\n",
      " [   21    52     3   556     2]\n",
      " [    2   199     5  2329   381]\n",
      " [    2   381    13    17  2109]\n",
      " [  857     8 26980     2    51]\n",
      " [    3     1    99     5     1]]\n"
     ]
    }
   ],
   "source": [
    "for i, (w, y) in enumerate(batch_generator(X_train, y_train, 10, 5)):\n",
    "    print y\n",
    "    print w\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "#        predictions_op = tf.no_op()\n",
    "        loss = lm.loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "#        predictions_op = lm.predictions_\n",
    "        loss = lm.loss_\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            \n",
    "#        print y\n",
    "#        print y[:,max_time-1].reshape(50,1)\n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y,\n",
    "                     lm.initial_h_: h,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout}\n",
    "        \n",
    "#        _, h, cost, = session.run([train_op, lm.final_h_, lm.loss_], feed_dict)  \n",
    "        _, h, logits, target_y, cost = session.run([train_op, lm.final_h_, lm.logits_last_, lm.target_y_last_, loss], feed_dict)  \n",
    "#        print \"logits shape\", logits.shape\n",
    "#        print \"target_y shape\", target_y.shape\n",
    "#        print target_y\n",
    "        \n",
    "#        break\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, authors, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = batch_generator(ids, authors, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 4\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    num_classes=num_classes,\n",
    "                    softmax_ns=7,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:35\n",
      "[epoch 1] Federalist Paper 18: avg. loss: 10.523  (perplexity: 37176.74)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:00:35\n",
      "[epoch 2] Federalist Paper 18: avg. loss: 8.955  (perplexity: 7743.93)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:00:35\n",
      "[epoch 3] Federalist Paper 18: avg. loss: 6.770  (perplexity: 871.13)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:00:33\n",
      "[epoch 4] Federalist Paper 18: avg. loss: 7.401  (perplexity: 1637.64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(X_train, y_train, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        # Run a training epoch.\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=100, learning_rate=learning_rate)\n",
    "    \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, eval_X['18'], eval_y['18'], name=\"Federalist Paper 18\")\n",
    "        #score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
