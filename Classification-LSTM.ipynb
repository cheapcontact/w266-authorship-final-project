{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# RNNLM Model\n",
    "import rnnlm\n",
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary class holds the vocabulary and the mapping between words and ids for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "'''Vocabulary class, nearly identical to that used in a4'''\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to massage the cleaned data into indivdual words. Punctuation at the end of a word is split into its own distince word. Also accomplishes some minor data cleaning, removing nonsense characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    rep_dict = {'\\n':' ', '\\x10': ' ', '\\x11': ' ', '\\x12': ' ', '\\x13': ' ', '\\x14': ' ', '\\x15': ' ', '\\x16': ' ', '\\x17': ' ', '\\x18': ' ', '\\x19': ' ', '\\x1a': ' ', '\\x1b': ' ', '\\x1c': ' ', '\\x1d': ' ', '\\x1e': ' ', '\\x1f': ' ', '\\x20': ' ', '\\x21': ' ', '\\x22': ' ', '\\x23': ' ', '\\x24': ' ', '\\x25': ' ', '\\x26': ' ', '\\x27': ' ', '\\x28': ' ', '\\x29': ' ', '\\x2a': ' ', '\\x2b': ' ', '\\x2c': ' ', '\\x2d': ' ', '\\x2e': ' ', '\\x2f': ' ', '\\x30': ' ', '\\x31': ' ', '\\x32': ' ', '\\x33': ' ', '\\x34': ' ', '\\x35': ' ', '\\x36': ' ', '\\x37': ' ', '\\x38': ' ', '\\x39': ' ', '\\x3a': ' ', '\\x3b': ' ', '\\x3c': ' ', '\\x3d': ' ', '\\x3e': ' ', '\\x3f': ' ', '\\x40': ' ', '\\x41': ' ', '\\x42': ' ', '\\x43': ' ', '\\x44': ' ', '\\x45': ' ', '\\x46': ' ', '\\x47': ' ', '\\x48': ' ', '\\x49': ' ', '\\x4a': ' ', '\\x4b': ' ', '\\x4c': ' ', '\\x4d': ' ', '\\x4e': ' ', '\\x4f': ' ', '\\x50': ' ', '\\x51': ' ', '\\x52': ' ', '\\x53': ' ', '\\x54': ' ', '\\x55': ' ', '\\x56': ' ', '\\x57': ' ', '\\x58': ' ', '\\x59': ' ', '\\x5a': ' ', '\\x5b': ' ', '\\x5c': ' ', '\\x5d': ' ', '\\x5e': ' ', '\\x5f': ' ', '\\x60': ' ', '\\x61': ' ', '\\x62': ' ', '\\x63': ' ', '\\x64': ' ', '\\x65': ' ', '\\x66': ' ', '\\x67': ' ', '\\x68': ' ', '\\x69': ' ', '\\x6a': ' ', '\\x6b': ' ', '\\x6c': ' ', '\\x6d': ' ', '\\x6e': ' ', '\\x6f': ' ', '\\x70': ' ', '\\x71': ' ', '\\x72': ' ', '\\x73': ' ', '\\x74': ' ', '\\x75': ' ', '\\x76': ' ', '\\x77': ' ', '\\x78': ' ', '\\x79': ' ', '\\x7a': ' ', '\\x7b': ' ', '\\x7c': ' ', '\\x7d': ' ', '\\x7e': ' ', '\\x7f': ' ', '\\x80': ' ', '\\x81': ' ', '\\x82': ' ', '\\x83': ' ', '\\x84': ' ', '\\x85': ' ', '\\x86': ' ', '\\x87': ' ', '\\x88': ' ', '\\x89': ' ', '\\x8a': ' ', '\\x8b': ' ', '\\x8c': ' ', '\\x8d': ' ', '\\x8e': ' ', '\\x8f': ' ', '\\x90': ' ', '\\x91': ' ', '\\x92': ' ', '\\x93': ' ', '\\x94': ' ', '\\x95': ' ', '\\x96': ' ', '\\x97': ' ', '\\x98': ' ', '\\x99': ' ', '\\x9a': ' ', '\\x9b': ' ', '\\x9c': ' ', '\\x9d': ' ', '\\x9e': ' ', '\\x9f': ' ', '\\xa0': ' ', '\\xa1': ' ', '\\xa2': ' ', '\\xa3': ' ', '\\xa4': ' ', '\\xa5': ' ', '\\xa6': ' ', '\\xa7': ' ', '\\xa8': ' ', '\\xa9': ' ', '\\xaa': ' ', '\\xab': ' ', '\\xac': ' ', '\\xad': ' ', '\\xae': ' ', '\\xaf': ' ', '\\xb0': ' ', '\\xb1': ' ', '\\xb2': ' ', '\\xb3': ' ', '\\xb4': ' ', '\\xb5': ' ', '\\xb6': ' ', '\\xb7': ' ', '\\xb8': ' ', '\\xb9': ' ', '\\xba': ' ', '\\xbb': ' ', '\\xbc': ' ', '\\xbd': ' ', '\\xbe': ' ', '\\xbf': ' ', '\\xc0': ' ', '\\xc1': ' ', '\\xc2': ' ', '\\xc3': ' ', '\\xc4': ' ', '\\xc5': ' ', '\\xc6': ' ', '\\xc7': ' ', '\\xc8': ' ', '\\xc9': ' ', '\\xca': ' ', '\\xcb': ' ', '\\xcc': ' ', '\\xcd': ' ', '\\xce': ' ', '\\xcf': ' ', '\\xd0': ' ', '\\xd1': ' ', '\\xd2': ' ', '\\xd3': ' ', '\\xd4': ' ', '\\xd5': ' ', '\\xd6': ' ', '\\xd7': ' ', '\\xd8': ' ', '\\xd9': ' ', '\\xda': ' ', '\\xdb': ' ', '\\xdc': ' ', '\\xdd': ' ', '\\xde': ' ', '\\xdf': ' ', '\\xe0': ' ', '\\xe1': ' ', '\\xe2': ' ', '\\xe3': ' ', '\\xe4': ' ', '\\xe5': ' ', '\\xe6': ' ', '\\xe7': ' ', '\\xe8': ' ', '\\xe9': ' ', '\\xea': ' ', '\\xeb': ' ', '\\xec': ' ', '\\xed': ' ', '\\xee': ' ', '\\xef': ' ', '\\xf0': ' ', '\\xf1': ' ', '\\xf2': ' ', '\\xf3': ' ', '\\xf4': ' ', '\\xf5': ' ', '\\xf6': ' ', '\\xf7': ' ', '\\xf8': ' ', '\\xf9': ' ', '\\xfa': ' ', '\\xfb': ' ', '\\xfc': ' ', '\\xfd': ' ', '\\xfe': ' ', '\\xff': ' '}\n",
    "    for word in replace_all(words, rep_dict).split(' '):   \n",
    "        if word and word[-1] in ('.', ',', '?', ';', '!'):\n",
    "            punk = word[-1]\n",
    "            current.append(punk)\n",
    "            word = word[0:-1]\n",
    "\n",
    "        word = canonicalize_word(word)\n",
    "        current.append(word)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_train_data reads in everything in train_data directory from which it generates the vocab (of type Vocabulary defined above), the author_to_id map (which maps author names to ids) and two arrays, one contains the text of each file (as a list of word ids) under \"train_data\". The other holds the author id for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        print author, author_id\n",
    "\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(np.array(current))\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    # replace words with ids\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = np.array(vocab.words_to_ids(x))\n",
    "\n",
    "    return vocab, np.array(X), np.array(y), author_to_id\n",
    "\n",
    "\n",
    "def id_to_author(author_to_id, id):\n",
    "    for author, author_id in author_to_id.iteritems():\n",
    "        if id == author_id:\n",
    "            return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the training data, note resulting number of classes (authors) and display some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thomas_paine 0\n",
      "thomas_jefferson 1\n",
      "john_adams 2\n",
      "james_madison 3\n",
      "alexander_hamilton 4\n",
      "james_monroe 5\n",
      "john_jay 6\n",
      "george_washington 7\n",
      "benjamin_franklin 8\n",
      "vocab.size 52056\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "vocab, X_train, y_train, author_to_id = load_train_data()\n",
    "num_classes = len(np.unique(y_train))\n",
    "print \"vocab.size\", vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the eval data in the same format as the training data. But each federalist paper here ends up in its own dictionary entry so that they can be scored/classified/attributed separately. Each is assumed to be written by James Madison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[[ 181    1 4285 ...,  395    5  497]]\n",
      "Who wrote Federalist paper 5 (John Jay should be answer): john_jay\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(vocab, eval_data_dir):\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        expanded_X = np.array(current)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = np.array([expanded_X])\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        if eval_data_dir == \"unknown_data\":\n",
    "           eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "        else:\n",
    "            if id == '5':\n",
    "                eval_y[id] = np.array([author_to_id['john_jay']])\n",
    "            elif id == '39':\n",
    "                eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "            else:\n",
    "                eval_y[id] = np.array([author_to_id['alexander_hamilton']])\n",
    "                \n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(vocab, \"unknown_data\")\n",
    "print eval_y['18']\n",
    "print eval_X['18']\n",
    "\n",
    "test_X, test_y = load_eval_data(vocab, \"test_data\")\n",
    "print \"Who wrote Federalist paper 5 (John Jay should be answer): %s\" % id_to_author(author_to_id, test_y['5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut up the publications based on batch_size, and max_time. To reduce how much code had to be changed from a4 this expands the author id for each document to be an author id for each word in the document. So if you had publication[1] = [1 2 3] and authors[1] = 1, this would output (assuming a batch of 1 and max time of 3) w = [1 2 3] and y = [1 1 1]. In the end we'll ignore all the loss for everything except the last word, but all the matrix functions and multiplications could work as is if I kept expanded the author to be associated with each word (since that is what the sequence math was doing, each word had a corresponding target word). Note: this also randomly shuffles the batches so that an given author's data is mixed through out the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# maybe worth seeding here. or in the batch_generator function\n",
    "\n",
    "def batch_generator(publications, authors, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form.\"\"\"\n",
    "    all_w = []\n",
    "    all_y = []\n",
    "    for i, ids in enumerate(publications):\n",
    "        # Clip to multiple of max_time for convenience\n",
    "        clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
    "        \n",
    "        input_w = ids[:clip_len]     # current word\n",
    "        target_y = ids[1:clip_len+1]  # next word\n",
    "        # Reshape so we can select columns\n",
    "        input_w = input_w.reshape([batch_size,-1])\n",
    "\n",
    "        for j in xrange(0, input_w.shape[1], max_time):\n",
    "            this_w = input_w[:,j:j+max_time]\n",
    "            all_w.append(this_w)\n",
    "            all_y.append(np.full_like(this_w, authors[i]))\n",
    "\n",
    "    # Yield batches in random order     \n",
    "    data = range(0, len(all_y)-1)\n",
    "    random.shuffle(data)   \n",
    "\n",
    "    for k in data:\n",
    "        yield all_w[k], all_y[k]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]]\n",
      "[[ 1117     3     1  2313   285]\n",
      " [ 2491     1  2221    52     3]\n",
      " [  176     3     2   118    16]\n",
      " [  651    12    43    11   117]\n",
      " [  237     1   227  2099   105]\n",
      " [    2   159     6     4    11]\n",
      " [   23     5  1554     1   674]\n",
      " [12076    12    43    19    49]\n",
      " [ 1016     3     2    12    20]\n",
      " [    3   595     2   196   105]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "for i, (w, y) in enumerate(batch_generator(X_train, y_train, 10, 5)):\n",
    "    print y\n",
    "    print w \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the epoch. Mostly the same as a4, but there is no test phase (instead there is no a separate prediction phase) that didn't makes as much sense to me to have in this function, so it has its own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y,\n",
    "                     lm.initial_h_: h,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout}\n",
    "        \n",
    "        _, h, cost = session.run([train_op, lm.final_h_, lm.loss_], feed_dict)  \n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, authors, max_time, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up. Same as a4\n",
    "    bi = batch_generator(ids, authors, batch_size=100, max_time=max_time)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to predict a batch iterators data. Used post training to test the unknown federalist papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Summary:\n",
      "thomas_jefferson: 0.50\n",
      "john_adams: 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def print_prediction_results(predictions, author_to_id):\n",
    "    counts = defaultdict(float)\n",
    "    for p in predictions:\n",
    "        counts[p] += 1\n",
    "\n",
    "    print \"Prediction Summary:\"\n",
    "    for id, count in counts.iteritems():\n",
    "        print \"%s: %.2f\" % (id_to_author(author_to_id, id), count/len(predictions))\n",
    "    print \"\"\n",
    "    \n",
    "def predict_paper(lm, session, batch_iterator, authors, paper_name):\n",
    "    total_predictions = np.array([])\n",
    "    print \"Predicting for %s\" % paper_name\n",
    "    for i, (w, y) in enumerate(batch_iterator):        \n",
    "        if i == 0:\n",
    "            print \"Truth:\", id_to_author(authors, y[0][0])\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y}\n",
    "        \n",
    "\n",
    "        \n",
    "        cost, truths, logits, predictions = session.run([lm.loss_, lm.target_y_last_, lm.logits_last_, lm.predictions_], feed_dict)  \n",
    "        total_predictions = np.append(total_predictions, predictions.reshape(-1))\n",
    "\n",
    "    print_prediction_results(total_predictions, authors)\n",
    "    \n",
    "print_prediction_results([1, 2, 1, 2], author_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 15\n",
    "batch_size = 40\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    num_classes=num_classes,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same as a4, but instead of scoring the data we look at prediction results after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(rnnlm)\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildClassifierGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(X_train, y_train, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        # Run a training epoch.\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=100, learning_rate=learning_rate)\n",
    "    \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, eval_X['18'], eval_y['18'], max_time, name=\"Federalist Paper 18\")\n",
    "        #score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "\n",
    "        # test three of the federalist papers whose author is known after each epoch\n",
    "        for key in test_X:\n",
    "            prediction_bi = batch_generator(test_X[key], test_y[key], batch_size, max_time)\n",
    "            predict_paper(lm, session, prediction_bi, author_to_id, \"Federalist Paper %s\" % key)        \n",
    "        print \"\"\n",
    "\n",
    "    # test on disputed papers at the end of all epochs\n",
    "    for key in eval_X:\n",
    "        prediction_bi = batch_generator(eval_X[key], eval_y[key], batch_size, max_time)\n",
    "        predict_paper(lm, session, prediction_bi, author_to_id, \"Federalist Paper %s\" % key)          \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "print author_to_id['james_madison']\n",
    "print author_to_id['george_washington']\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
