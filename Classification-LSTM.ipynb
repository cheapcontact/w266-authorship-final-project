{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# RNNLM Model\n",
    "import rnnlm\n",
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary class holds the vocabulary and the mapping between words and ids for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "'''Vocabulary class, nearly identical to that used in a4'''\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to massage the cleaned data into indivdual words. Punctuation at the end of a word is split into its own distince word. Also accomplishes some minor data cleaning, removing nonsense characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    rep_dict = {'\\n':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xa0':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xc3':' '\n",
    "                ,'\\xa9':' '\n",
    "                ,'\\xef':' '\n",
    "                ,'\\xbb':' '\n",
    "                ,'\\xbf':' '\n",
    "                ,'\\xa6':' '\n",
    "                ,'\\xb9':' '\n",
    "                ,'\\xa3':' '\n",
    "                ,'\\xbd':' '\n",
    "                ,'\\xb4':' '\n",
    "                ,'\\xcb':' '\n",
    "                ,'\\x9a':' '\n",
    "                ,'\\x86':' '\n",
    "                ,'\\xcf':' '\n",
    "                ,'\\x84':' '\n",
    "                ,'\\xce':' '\n",
    "                ,'\\x87':' '\n",
    "                ,'\\xe2':' '\n",
    "                ,'\\x80':' '\n",
    "                ,'\\x94':' '\n",
    "               }\n",
    "    for word in replace_all(words, rep_dict).split(' '):   \n",
    "        if word:\n",
    "            if word[-1] in ('.', ',', '?', ';', '!'):\n",
    "                punk = word[-1]\n",
    "                current.append(punk)\n",
    "                word = word[0:-1]\n",
    "\n",
    "            word = canonicalize_word(word)\n",
    "            current.append(word)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_train_data reads in everything in train_data directory from which it generates the vocab (of type Vocabulary defined above), the author_to_id map (which maps author names to ids) and two arrays, one contains the text of each file (as a list of word ids) under \"train_data\". The other holds the author id for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        print author, author_id\n",
    "\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(np.array(current))\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    # replace words with ids\n",
    "    for i, x in enumerate(X):\n",
    "        # X[i] = np.array(x) # This line can be used to make sure your words are useful \n",
    "        X[i] = np.array(vocab.words_to_ids(x))\n",
    "\n",
    "    return vocab, np.array(X), np.array(y), author_to_id\n",
    "\n",
    "\n",
    "def id_to_author(author_to_id, id):\n",
    "    for author, author_id in author_to_id.iteritems():\n",
    "        if id == author_id:\n",
    "            return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the training data, note resulting number of classes (authors) and display some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thomas_paine 0\n",
      "thomas_jefferson 1\n",
      "john_adams 2\n",
      "james_madison 3\n",
      "alexander_hamilton 4\n",
      "james_monroe 5\n",
      "john_jay 6\n",
      "george_washington 7\n",
      "benjamin_franklin 8\n",
      "vocab.size 52056\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "vocab, X_train, y_train, author_to_id = load_train_data()\n",
    "num_classes = len(np.unique(y_train))\n",
    "print \"vocab.size\", vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the eval data in the same format as the training data. But each federalist paper here ends up in its own dictionary entry so that they can be scored/classified/attributed separately. Each is assumed to be written by James Madison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[[ 180    1 4285 ...,  395    5  497]]\n",
      "Who wrote Federalist paper 5 (John Jay should be answer): john_jay\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(vocab, eval_data_dir):\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        expanded_X = np.array(current)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = np.array([expanded_X])\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        if eval_data_dir == \"unknown_data\":\n",
    "            eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "        else:\n",
    "            if id == '5':\n",
    "                eval_y[id] = np.array([author_to_id['john_jay']])\n",
    "            elif id == '39':\n",
    "                eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "            else:\n",
    "                eval_y[id] = np.array([author_to_id['alexander_hamilton']])\n",
    "                \n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(vocab, \"unknown_data\")\n",
    "print eval_y['18']\n",
    "print eval_X['18']\n",
    "\n",
    "test_X, test_y = load_eval_data(vocab, \"test_data\")\n",
    "print \"Who wrote Federalist paper 5 (John Jay should be answer): %s\" % id_to_author(author_to_id, test_y['5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut up the publications based on batch_size, and max_time. To reduce how much code had to be changed from a4 this expands the author id for each document to be an author id for each word in the document. So if you had publication[1] = [1 2 3] and authors[1] = 1, this would output (assuming a batch of 1 and max time of 3) w = [1 2 3] and y = [1 1 1]. In the end we'll ignore all the loss for everything except the last word, but all the matrix functions and multiplications could work as is if I kept expanded the author to be associated with each word (since that is what the sequence math was doing, each word had a corresponding target word). Note: this also randomly shuffles the batches so that an given author's data is mixed through out the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'I', 'am'], ['mr', '.', 'anderson'], ['what', 'is', 'your']]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_up_words(words, window_size=10, step_size=1):\n",
    "    clip_len = ((len(words)-1) / window_size) * window_size\n",
    "    words = words[:clip_len]\n",
    "    slices = []\n",
    "    num_words = len(words)\n",
    "    for index in range(0, num_words, step_size):\n",
    "        slices.append(words[index:index+window_size])\n",
    "    return slices\n",
    "        \n",
    "slice_up_words([\"hello\", \"I\", \"am\", \"mr\", \".\", \"anderson\", \"what\", \"is\", \"your\", \"name\", \"?\"], 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# maybe worth seeding here. or in the batch_generator function\n",
    "\n",
    "def shape_data_for_batching(publications, authors, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form.\"\"\"\n",
    "    all_w = None\n",
    "    all_y = []\n",
    "    for i, ids in enumerate(publications):\n",
    "        ids = np.array(slice_up_words(ids, max_time, max_time))\n",
    "        y = np.full_like(ids, authors[i])\n",
    "        \n",
    "        if all_w is not None:\n",
    "            all_w = np.append(all_w, ids, 0)\n",
    "            all_y = np.append(all_y, y, 0)\n",
    "            \n",
    "        else:\n",
    "            all_w = ids\n",
    "            all_y = y\n",
    "\n",
    "    # Yield batches in random order     \n",
    "    index = range(0, len(all_y)-1)\n",
    "    random.shuffle(index)   \n",
    "\n",
    "    all_w = [all_w[i] for i in index]\n",
    "    all_y = [all_y[i] for i in index]\n",
    "\n",
    "    return all_w, all_y\n",
    "\n",
    "def batch_generator(X_shaped, y_shaped, batch_size):\n",
    "    clip_len = ((len(X_shaped)-1) / batch_size) * batch_size\n",
    "    X_shaped = X_shaped[:clip_len]\n",
    "    y_shaped = y_shaped[:clip_len]  \n",
    "    for j in xrange(0, len(X_shaped), batch_size):\n",
    "        this_x = X_shaped[j:j+batch_size]\n",
    "        this_y = y_shaped[j:j+batch_size]\n",
    "        yield np.array(this_x), np.array(this_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "X_shaped, y_shaped = shape_data_for_batching(X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 4 4 4 4]\n",
      " [4 4 4 4 4]\n",
      " [4 4 4 4 4]\n",
      " [4 4 4 4 4]\n",
      " [3 3 3 3 3]\n",
      " [1 1 1 1 1]\n",
      " [4 4 4 4 4]\n",
      " [4 4 4 4 4]\n",
      " [3 3 3 3 3]\n",
      " [0 0 0 0 0]]\n",
      "[[ 5756     6     5 14037   207]\n",
      " [ 3702  4209     3     5   394]\n",
      " [ 2627  1309     3     1     2]\n",
      " [    1  2897    60     4   120]\n",
      " [   60  1104    20     1  1261]\n",
      " [   38     2  1010  8299     9]\n",
      " [   20    53   578   774     3]\n",
      " [   14     2  4466     7   485]\n",
      " [   11    28   600   258  1315]\n",
      " [   11    13     5   126    12]]\n"
     ]
    }
   ],
   "source": [
    "for i, (w, y) in enumerate(batch_generator(X_shaped, y_shaped, 10)):\n",
    "    print y\n",
    "    print w \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the epoch. Mostly the same as a4, but there is no test phase (instead there is no a separate prediction phase) that didn't makes as much sense to me to have in this function, so it has its own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y,\n",
    "                     lm.initial_h_: h,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout}\n",
    "        \n",
    "        _, h, cost = session.run([train_op, lm.final_h_, lm.loss_], feed_dict)  \n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, authors, max_time, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up. Same as a4\n",
    "    bi = batch_generator(ids, authors, batch_size=100, max_time=max_time)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to predict a batch iterators data. Used post training to test the unknown federalist papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Summary:\n",
      "thomas_jefferson: 0.50\n",
      "john_adams: 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def print_prediction_results(predictions, author_to_id):\n",
    "    counts = defaultdict(float)\n",
    "    for p in predictions:\n",
    "        counts[p] += 1\n",
    "\n",
    "    print \"Prediction Summary:\"\n",
    "    for id, count in counts.iteritems():\n",
    "        print \"%s: %.2f\" % (id_to_author(author_to_id, id), count/len(predictions))\n",
    "    print \"\"\n",
    "    \n",
    "def predict_paper(lm, session, batch_iterator, authors, paper_name):\n",
    "    total_predictions = np.array([])\n",
    "    print \"Predicting for %s\" % paper_name\n",
    "    for i, (w, y) in enumerate(batch_iterator):        \n",
    "        if i == 0:\n",
    "            print \"Truth:\", id_to_author(authors, y[0][0])\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y}\n",
    "        \n",
    "\n",
    "        \n",
    "        cost, truths, logits, predictions = session.run([lm.loss_, lm.target_y_last_, lm.logits_last_, lm.predictions_], feed_dict)  \n",
    "        total_predictions = np.append(total_predictions, predictions.reshape(-1))\n",
    "\n",
    "    print_prediction_results(total_predictions, authors)\n",
    "    \n",
    "print_prediction_results([1, 2, 1, 2], author_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 15\n",
    "batch_size = 40\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    num_classes=num_classes,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")\n",
    "\n",
    "\n",
    "X_train_shaped, y_train_shaped = shape_data_for_batching(X_train, y_train, max_time)\n",
    "\n",
    "X_test_shaped = {}\n",
    "y_test_shaped = {}\n",
    "for key in test_X:\n",
    "    X_test_shaped[key], y_test_shaped[key] = shape_data_for_batching(test_X[key], test_y[key], max_time)\n",
    "\n",
    "X_eval_shaped = {}\n",
    "y_eval_shaped = {}\n",
    "for key in eval_X:\n",
    "    X_eval_shaped[key], y_eval_shaped[key] = shape_data_for_batching(eval_X[key], eval_y[key], max_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same as a4, but instead of scoring the data we look at prediction results after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 5485]: seen 3291600 words at 32914 wps, loss = 1.388\n",
      "[epoch 1] Completed in 0:01:56\n",
      "[epoch 1] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.57\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.28\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.61\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.27\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.82\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.15\n",
      "\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 5479]: seen 3288000 words at 32877 wps, loss = 1.317\n",
      "[epoch 2] Completed in 0:01:56\n",
      "[epoch 2] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.44\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.50\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.42\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.55\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.64\n",
      "alexander_hamilton: 0.36\n",
      "\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 5376]: seen 3226200 words at 32259 wps, loss = 1.282\n",
      "[epoch 3] Completed in 0:01:58\n",
      "[epoch 3] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.39\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.53\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.62\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.59\n",
      "alexander_hamilton: 0.41\n",
      "\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 5372]: seen 3223800 words at 32236 wps, loss = 1.260\n",
      "[epoch 4] Completed in 0:01:58\n",
      "[epoch 4] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.38\n",
      "james_madison: 0.09\n",
      "alexander_hamilton: 0.53\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.32\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.64\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.56\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.41\n",
      "\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 5680]: seen 3408600 words at 34081 wps, loss = 1.244\n",
      "[epoch 5] Completed in 0:01:52\n",
      "[epoch 5] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.37\n",
      "james_madison: 0.09\n",
      "alexander_hamilton: 0.54\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.32\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.62\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.60\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.36\n",
      "\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "[batch 5642]: seen 3385800 words at 33855 wps, loss = 1.229\n",
      "[epoch 6] Completed in 0:01:53\n",
      "[epoch 6] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.32\n",
      "james_madison: 0.13\n",
      "alexander_hamilton: 0.55\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.62\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.55\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.42\n",
      "\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "[batch 5602]: seen 3361800 words at 33613 wps, loss = 1.217\n",
      "[epoch 7] Completed in 0:01:53\n",
      "[epoch 7] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.14\n",
      "alexander_hamilton: 0.58\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.29\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.63\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.54\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.42\n",
      "\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "[batch 5637]: seen 3382800 words at 33822 wps, loss = 1.207\n",
      "[epoch 8] Completed in 0:01:53\n",
      "[epoch 8] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.29\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.59\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.24\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.71\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.55\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.41\n",
      "\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "[batch 5635]: seen 3381600 words at 33811 wps, loss = 1.197\n",
      "[epoch 9] Completed in 0:01:53\n",
      "[epoch 9] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.14\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.24\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.49\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "[batch 5666]: seen 3400200 words at 33998 wps, loss = 1.189\n",
      "[epoch 10] Completed in 0:01:52\n",
      "[epoch 10] Predicting for Federalist Paper 39\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.55\n",
      "\n",
      "Predicting for Federalist Paper 30\n",
      "Truth: alexander_hamilton\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.02\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 5\n",
      "Truth: john_jay\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.51\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.46\n",
      "\n",
      "\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.54\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.36\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.25\n",
      "alexander_hamilton: 0.42\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.26\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.56\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.41\n",
      "james_madison: 0.14\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.22\n",
      "alexander_hamilton: 0.51\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.37\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.46\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.34\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.24\n",
      "james_madison: 0.30\n",
      "alexander_hamilton: 0.46\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.40\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.39\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.27\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.53\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.35\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.48\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.55\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.37\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.36\n",
      "james_madison: 0.18\n",
      "alexander_hamilton: 0.46\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.40\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_paine: 0.01\n",
      "thomas_jefferson: 0.65\n",
      "james_madison: 0.11\n",
      "alexander_hamilton: 0.23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildClassifierGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(X_train_shaped, y_train_shaped, batch_size)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        # Run a training epoch.\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=100, learning_rate=learning_rate)\n",
    "    \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, eval_X['18'], eval_y['18'], max_time, name=\"Federalist Paper 18\")\n",
    "        #score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "\n",
    "        # test three of the federalist papers whose author is known after each epoch\n",
    "        for key in X_test_shaped:\n",
    "            prediction_bi = batch_generator(X_test_shaped[key], y_test_shaped[key], batch_size)\n",
    "            predict_paper(lm, session, prediction_bi, author_to_id, \"Federalist Paper %s\" % key)        \n",
    "        print \"\"\n",
    "\n",
    "    # test on disputed papers at the end of all epochs\n",
    "    for key in X_eval_shaped:\n",
    "        prediction_bi = batch_generator(X_eval_shaped[key], y_eval_shaped[key], batch_size)\n",
    "        predict_paper(lm, session, prediction_bi, author_to_id, \"Federalist Paper %s\" % key)          \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "print author_to_id['james_madison']\n",
    "print author_to_id['george_washington']\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
