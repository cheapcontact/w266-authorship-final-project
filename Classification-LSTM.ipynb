{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# RNNLM Model\n",
    "import rnnlm\n",
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary class holds the vocabulary and the mapping between words and ids for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "'''Vocabulary class, nearly identical to that used in a4'''\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to massage the cleaned data into indivdual words. Punctuation at the end of a word is split into its own distince word. Also accomplishes some minor data cleaning, removing nonsense characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    rep_dict = {'\\n':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xa0':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xc3':' '\n",
    "                ,'\\xa9':' '\n",
    "                ,'\\xef':' '\n",
    "                ,'\\xbb':' '\n",
    "                ,'\\xbf':' '\n",
    "               }\n",
    "    for word in replace_all(words, rep_dict).split(' '):   \n",
    "        if word and word[-1] in ('.', ',', '?', ';', '!'):\n",
    "            punk = word[-1]\n",
    "            current.append(punk)\n",
    "            word = word[0:-1]\n",
    "\n",
    "        word = canonicalize_word(word)\n",
    "        current.append(word)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_train_data reads in everything in train_data directory from which it generates the vocab (of type Vocabulary defined above), the author_to_id map (which maps author names to ids) and two arrays, one contains the text of each file (as a list of word ids) under \"train_data\". The other holds the author id for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        print author, author_id\n",
    "\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(np.array(current))\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    # replace words with ids\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = np.array(vocab.words_to_ids(x))\n",
    "\n",
    "    return vocab, np.array(X), np.array(y), author_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the training data, note resulting number of classes (authors) and display some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thomas_paine 0\n",
      "thomas_jefferson 1\n",
      "john_adams 2\n",
      "james_madison 3\n",
      "alexander_hamilton 4\n",
      "james_monroe 5\n",
      "john_jay 6\n",
      "george_washington 7\n",
      "benjamin_franklin 8\n",
      "vocab.size 142863\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "vocab, X_train, y_train, author_to_id = load_train_data()\n",
    "num_classes = len(np.unique(y_train))\n",
    "print \"vocab.size\", vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the eval data in the same format as the training data. But each federalist paper here ends up in its own dictionary entry so that they can be scored/classified/attributed separately. Each is assumed to be written by James Madison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[[ 185    2 3794 ...,  365    5  497]]\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(vocab):\n",
    "    eval_data_dir = \"unknown_data\"\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        expanded_X = np.array(current)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = np.array([expanded_X])\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "\n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(vocab)\n",
    "print eval_y['18']\n",
    "print eval_X['18']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut up the publications based on batch_size, and max_time. To reduce how much code had to be changed from a4 this expands the author id for each document to be an author id for each word in the document. So if you had publication[1] = [1 2 3] and authors[1] = 1, this would output (assuming a batch of 1 and max time of 3) w = [1 2 3] and y = [1 1 1]. In the end we'll ignore all the loss for everything except the last word, but all the matrix functions and multiplications could work as is if I kept expanded the author to be associated with each word (since that is what the sequence math was doing, each word had a corresponding target word). Note: this also randomly shuffles the batches so that an given author's data is mixed through out the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# maybe worth seeding here. or in the batch_generator function\n",
    "\n",
    "def batch_generator(publications, authors, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form.\"\"\"\n",
    "    all_w = []\n",
    "    all_y = []\n",
    "    for i, ids in enumerate(publications):\n",
    "        # Clip to multiple of max_time for convenience\n",
    "        clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
    "        \n",
    "        input_w = ids[:clip_len]     # current word\n",
    "        target_y = ids[1:clip_len+1]  # next word\n",
    "        # Reshape so we can select columns\n",
    "        input_w = input_w.reshape([batch_size,-1])\n",
    "\n",
    "        for j in xrange(0, input_w.shape[1], max_time):\n",
    "            this_w = input_w[:,j:j+max_time]\n",
    "            all_w.append(this_w)\n",
    "            all_y.append(np.full_like(this_w, authors[i]))\n",
    "\n",
    "    # Yield batches in random order     \n",
    "    data = range(0, len(all_y)-1)\n",
    "    random.shuffle(data)   \n",
    "\n",
    "    for k in data:\n",
    "        yield all_w[k], all_y[k]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 1 1 1]]\n",
      "[[     2    454     39     49      8]\n",
      " [    10     35      2   4870      3]\n",
      " [107567      5  41765     11     12]\n",
      " [    93     26      1     72      6]\n",
      " [   213     21  29107      1    129]\n",
      " [     1    695      2    340   3371]\n",
      " [ 55802    106    751      9      2]\n",
      " [     7     16      1    121     11]\n",
      " [  7357     13     64    354     27]\n",
      " [     1    243     11     12    574]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "for i, (w, y) in enumerate(batch_generator(X_train, y_train, 10, 5)):\n",
    "    print y\n",
    "    print w \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the epoch. Mostly the same as a4, but there is no test phase (instead there is no a separate prediction phase) that didn't makes as much sense to me to have in this function, so it has its own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y,\n",
    "                     lm.initial_h_: h,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout}\n",
    "        \n",
    "        _, h, cost = session.run([train_op, lm.final_h_, lm.loss_], feed_dict)  \n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, authors, max_time, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up. Same as a4\n",
    "    bi = batch_generator(ids, authors, batch_size=100, max_time=max_time)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to predict a batch iterators data. Used post training to test the unknown federalist papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.50\n",
      "john_adams: 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def print_prediction_results(predictions, author_to_id):\n",
    "    print \"Truth:\", 'james_madison'\n",
    "    counts = defaultdict(float)\n",
    "    for p in predictions:\n",
    "        counts[p] += 1\n",
    "\n",
    "    print \"Prediction Summary:\"\n",
    "    for id, count in counts.iteritems():\n",
    "        print \"%s: %.2f\" % (id_to_author(author_to_id, id), count/len(predictions))\n",
    "    print \"\"\n",
    "\n",
    "def id_to_author(author_to_id, id):\n",
    "    for author, author_id in author_to_id.iteritems():\n",
    "        if id == author_id:\n",
    "            return author\n",
    "    \n",
    "def predict_paper(lm, session, batch_iterator, authors, paper_name):\n",
    "    total_predictions = np.array([])\n",
    "    print \"Predicting for %s\" % paper_name\n",
    "    for i, (w, y) in enumerate(batch_iterator):        \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y}\n",
    "        \n",
    "        cost, truths, logits, predictions = session.run([lm.loss_, lm.target_y_last_, lm.logits_last_, lm.predictions_], feed_dict)  \n",
    "        total_predictions = np.append(total_predictions, predictions.reshape(-1))\n",
    "\n",
    "    print_prediction_results(total_predictions, authors)\n",
    "    \n",
    "print_prediction_results([1, 2, 1, 2], author_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 15\n",
    "batch_size = 40\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    num_classes=num_classes,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly the same as a4, but instead of score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 4916]: seen 2922960 words at 29225 wps, loss = 1.424\n",
      "[epoch 1] Completed in 0:02:02\n",
      "[epoch 1] Federalist Paper 18: avg. loss: 1.484  (perplexity: 4.41)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.72\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.09\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.59\n",
      "james_madison: 0.25\n",
      "alexander_hamilton: 0.16\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.49\n",
      "james_madison: 0.26\n",
      "alexander_hamilton: 0.25\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.67\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.14\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.55\n",
      "james_madison: 0.24\n",
      "alexander_hamilton: 0.21\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.53\n",
      "james_madison: 0.27\n",
      "alexander_hamilton: 0.20\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.57\n",
      "james_madison: 0.23\n",
      "alexander_hamilton: 0.20\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.52\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.21\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.65\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.20\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.60\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.19\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.62\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.19\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.69\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.13\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.57\n",
      "james_madison: 0.23\n",
      "alexander_hamilton: 0.19\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.61\n",
      "james_madison: 0.20\n",
      "alexander_hamilton: 0.19\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.72\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.13\n",
      "\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 5792]: seen 3443680 words at 34434 wps, loss = 1.348\n",
      "[epoch 2] Completed in 0:01:46\n",
      "[epoch 2] Federalist Paper 18: avg. loss: 1.516  (perplexity: 4.55)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.31\n",
      "james_madison: 0.09\n",
      "alexander_hamilton: 0.60\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.17\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.63\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.11\n",
      "james_madison: 0.20\n",
      "alexander_hamilton: 0.69\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.29\n",
      "james_madison: 0.09\n",
      "alexander_hamilton: 0.62\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.11\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.72\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.20\n",
      "james_madison: 0.09\n",
      "alexander_hamilton: 0.71\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.20\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.70\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.10\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.71\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.25\n",
      "james_madison: 0.11\n",
      "alexander_hamilton: 0.64\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.18\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.72\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.18\n",
      "james_madison: 0.13\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.60\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.18\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.67\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.20\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.70\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.38\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.56\n",
      "\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 5875]: seen 3494560 words at 34940 wps, loss = 1.315\n",
      "[epoch 3] Completed in 0:01:44\n",
      "[epoch 3] Federalist Paper 18: avg. loss: 1.537  (perplexity: 4.65)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.46\n",
      "james_madison: 0.25\n",
      "alexander_hamilton: 0.29\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.41\n",
      "alexander_hamilton: 0.29\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.19\n",
      "james_madison: 0.36\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.38\n",
      "james_madison: 0.24\n",
      "alexander_hamilton: 0.38\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.33\n",
      "alexander_hamilton: 0.44\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.27\n",
      "alexander_hamilton: 0.43\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.33\n",
      "alexander_hamilton: 0.38\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.19\n",
      "james_madison: 0.45\n",
      "alexander_hamilton: 0.36\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.40\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.33\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.27\n",
      "james_madison: 0.33\n",
      "alexander_hamilton: 0.41\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.31\n",
      "james_madison: 0.30\n",
      "alexander_hamilton: 0.39\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.46\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.26\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.32\n",
      "alexander_hamilton: 0.41\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.35\n",
      "james_madison: 0.27\n",
      "alexander_hamilton: 0.38\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.58\n",
      "james_madison: 0.20\n",
      "alexander_hamilton: 0.22\n",
      "\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 5862]: seen 3487040 words at 34865 wps, loss = 1.292\n",
      "[epoch 4] Completed in 0:01:45\n",
      "[epoch 4] Federalist Paper 18: avg. loss: 1.677  (perplexity: 5.35)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.57\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.30\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.36\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.25\n",
      "james_madison: 0.25\n",
      "alexander_hamilton: 0.50\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.43\n",
      "james_madison: 0.13\n",
      "alexander_hamilton: 0.43\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.20\n",
      "alexander_hamilton: 0.53\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.37\n",
      "james_madison: 0.13\n",
      "alexander_hamilton: 0.50\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.31\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.50\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.24\n",
      "james_madison: 0.24\n",
      "alexander_hamilton: 0.52\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.42\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.40\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.13\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.36\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.48\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.57\n",
      "james_madison: 0.13\n",
      "alexander_hamilton: 0.29\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.31\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.50\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.40\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.44\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.67\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.28\n",
      "\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 5730]: seen 3407600 words at 34070 wps, loss = 1.275\n",
      "[epoch 5] Completed in 0:01:47\n",
      "[epoch 5] Federalist Paper 18: avg. loss: 1.859  (perplexity: 6.42)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.45\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.51\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.21\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.75\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.17\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.79\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.02\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.18\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.73\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.26\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.66\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.14\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.76\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.61\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.22\n",
      "james_madison: 0.02\n",
      "alexander_hamilton: 0.77\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.25\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.69\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.51\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.43\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.26\n",
      "james_madison: 0.04\n",
      "alexander_hamilton: 0.71\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.29\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.52\n",
      "james_madison: 0.02\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "[batch 5665]: seen 3369160 words at 33691 wps, loss = 1.265\n",
      "[epoch 6] Completed in 0:01:48\n",
      "[epoch 6] Federalist Paper 18: avg. loss: 1.657  (perplexity: 5.24)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.47\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.40\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.26\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.16\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.62\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.25\n",
      "james_madison: 0.18\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.31\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.61\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.26\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.59\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.17\n",
      "james_madison: 0.24\n",
      "alexander_hamilton: 0.59\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.35\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.11\n",
      "alexander_hamilton: 0.66\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.61\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.44\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.40\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.26\n",
      "james_madison: 0.14\n",
      "alexander_hamilton: 0.60\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.32\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.52\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.54\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.33\n",
      "\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "[batch 5702]: seen 3391760 words at 33913 wps, loss = 1.247\n",
      "[epoch 7] Completed in 0:01:47\n",
      "[epoch 7] Federalist Paper 18: avg. loss: 1.702  (perplexity: 5.49)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.34\n",
      "james_madison: 0.09\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.17\n",
      "james_madison: 0.05\n",
      "alexander_hamilton: 0.78\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.14\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.80\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.27\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.67\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.11\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.79\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.24\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.70\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.11\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.77\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.10\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.21\n",
      "james_madison: 0.03\n",
      "alexander_hamilton: 0.76\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.08\n",
      "alexander_hamilton: 0.69\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.41\n",
      "james_madison: 0.11\n",
      "alexander_hamilton: 0.48\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.21\n",
      "james_madison: 0.06\n",
      "alexander_hamilton: 0.73\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.24\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.69\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.40\n",
      "james_madison: 0.07\n",
      "alexander_hamilton: 0.53\n",
      "\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "[batch 5825]: seen 3464840 words at 34646 wps, loss = 1.241\n",
      "[epoch 8] Completed in 0:01:45\n",
      "[epoch 8] Federalist Paper 18: avg. loss: 1.543  (perplexity: 4.68)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.49\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.15\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.11\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.61\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.60\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.10\n",
      "james_madison: 0.26\n",
      "alexander_hamilton: 0.64\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.59\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.16\n",
      "james_madison: 0.24\n",
      "alexander_hamilton: 0.60\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.12\n",
      "james_madison: 0.32\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.21\n",
      "james_madison: 0.24\n",
      "alexander_hamilton: 0.55\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.16\n",
      "james_madison: 0.18\n",
      "alexander_hamilton: 0.66\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.19\n",
      "james_madison: 0.23\n",
      "alexander_hamilton: 0.58\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.35\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.16\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.62\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.21\n",
      "james_madison: 0.23\n",
      "alexander_hamilton: 0.56\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.39\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "[batch 5767]: seen 3429240 words at 34291 wps, loss = 1.231\n",
      "[epoch 9] Completed in 0:01:46\n",
      "[epoch 9] Federalist Paper 18: avg. loss: 1.608  (perplexity: 4.99)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.53\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.29\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.23\n",
      "james_madison: 0.32\n",
      "alexander_hamilton: 0.45\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.16\n",
      "james_madison: 0.35\n",
      "alexander_hamilton: 0.49\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.35\n",
      "james_madison: 0.22\n",
      "alexander_hamilton: 0.43\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.17\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.54\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.33\n",
      "james_madison: 0.23\n",
      "alexander_hamilton: 0.44\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.25\n",
      "james_madison: 0.25\n",
      "alexander_hamilton: 0.50\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.17\n",
      "james_madison: 0.37\n",
      "alexander_hamilton: 0.46\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.35\n",
      "james_madison: 0.29\n",
      "alexander_hamilton: 0.36\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.25\n",
      "james_madison: 0.27\n",
      "alexander_hamilton: 0.48\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.24\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.48\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.49\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.30\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.49\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.30\n",
      "james_madison: 0.28\n",
      "alexander_hamilton: 0.41\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.57\n",
      "james_madison: 0.19\n",
      "alexander_hamilton: 0.24\n",
      "\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "[batch 5793]: seen 3445920 words at 34455 wps, loss = 1.213\n",
      "[epoch 10] Completed in 0:01:46\n",
      "[epoch 10] Federalist Paper 18: avg. loss: 1.611  (perplexity: 5.01)\n",
      "Predicting for Federalist Paper 20\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 58\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.13\n",
      "james_madison: 0.14\n",
      "alexander_hamilton: 0.72\n",
      "\n",
      "Predicting for Federalist Paper 49\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.09\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.75\n",
      "\n",
      "Predicting for Federalist Paper 55\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.21\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.67\n",
      "\n",
      "Predicting for Federalist Paper 54\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.07\n",
      "james_madison: 0.18\n",
      "alexander_hamilton: 0.74\n",
      "\n",
      "Predicting for Federalist Paper 57\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.20\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.68\n",
      "\n",
      "Predicting for Federalist Paper 56\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.16\n",
      "james_madison: 0.21\n",
      "alexander_hamilton: 0.62\n",
      "\n",
      "Predicting for Federalist Paper 51\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.07\n",
      "james_madison: 0.22\n",
      "alexander_hamilton: 0.71\n",
      "\n",
      "Predicting for Federalist Paper 50\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.21\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.61\n",
      "\n",
      "Predicting for Federalist Paper 53\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.13\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.74\n",
      "\n",
      "Predicting for Federalist Paper 52\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.16\n",
      "james_madison: 0.17\n",
      "alexander_hamilton: 0.67\n",
      "\n",
      "Predicting for Federalist Paper 19\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.28\n",
      "james_madison: 0.15\n",
      "alexander_hamilton: 0.57\n",
      "\n",
      "Predicting for Federalist Paper 62\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.13\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.75\n",
      "\n",
      "Predicting for Federalist Paper 63\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.18\n",
      "james_madison: 0.16\n",
      "alexander_hamilton: 0.66\n",
      "\n",
      "Predicting for Federalist Paper 18\n",
      "Truth: james_madison\n",
      "Prediction Summary:\n",
      "thomas_jefferson: 0.40\n",
      "james_madison: 0.12\n",
      "alexander_hamilton: 0.47\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildClassifierGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(X_train, y_train, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        # Run a training epoch.\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=100, learning_rate=learning_rate)\n",
    "    \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, eval_X['18'], eval_y['18'], max_time, name=\"Federalist Paper 18\")\n",
    "        #score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "\n",
    "        for key in eval_X:\n",
    "            prediction_bi = batch_generator(eval_X[key], eval_y[key], batch_size, max_time)\n",
    "            predict_paper(lm, session, prediction_bi, author_to_id, \"Federalist Paper %s\" % key)        \n",
    "        print \"\"\n",
    "\n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "print author_to_id['james_madison']\n",
    "print author_to_id['george_washington']\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
