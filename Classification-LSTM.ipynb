{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# RNNLM Model\n",
    "import rnnlm\n",
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    for word in words.split(\" \"):   \n",
    "        if word and word[-1] in (\".\", ',', '?', ';', '!'):\n",
    "            punk = word[-1]\n",
    "            current.append(punk)\n",
    "            word = word[0:-1]\n",
    "\n",
    "        word = canonicalize_word(word)\n",
    "        current.append(word)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            print full_path, author_id\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(np.array(current))\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    # replace words with ids\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = np.array(vocab.words_to_ids(x))\n",
    "\n",
    "    return vocab, np.array(X), np.array(y), author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data/thomas_paine/paine_rights_of_man_gutenberg_clean.txt 0\n",
      "train_data/thomas_paine/paine_american_crisis_gutenberg_clean.txt 0\n",
      "train_data/thomas_paine/paine_common_sense_gutenberg_clean.txt 0\n",
      "train_data/thomas_jefferson/jefferson_writings_of_vol_2_gutenberg_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_writings_of_vol_4_gutenberg_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_writings_of_vol_1_gutenberg_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_writings_of_vol_3_gutenberg_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_state_of_the_unions_gutenberg_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_writings_of_vol_5_gutenberg_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_writings_of_vol_7_gutenberg_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_misc_letters_addresses_clean.txt 1\n",
      "train_data/thomas_jefferson/jefferson_writings_of_vol_6_gutenberg_clean.txt 1\n",
      "train_data/john_adams/adams_state_of_the_unions_gutenberg_clean.txt 2\n",
      "train_data/james_madison/federalist_paper_39.txt 3\n",
      "train_data/james_madison/federalist_paper_48.txt 3\n",
      "train_data/james_madison/federalist_paper_40.txt 3\n",
      "train_data/james_madison/federalist_paper_45.txt 3\n",
      "train_data/james_madison/federalist_paper_47.txt 3\n",
      "train_data/james_madison/federalist_paper_42.txt 3\n",
      "train_data/james_madison/madison_examination_british_trade_doctrine_clean.txt 3\n",
      "train_data/james_madison/madison-the-writings-vol-1-1769-1783_clean.txt 3\n",
      "train_data/james_madison/madison_on_treaties_clean.txt 3\n",
      "train_data/james_madison/federalist_paper_10.txt 3\n",
      "train_data/james_madison/federalist_paper_46.txt 3\n",
      "train_data/james_madison/federalist_paper_14.txt 3\n",
      "train_data/james_madison/federalist_paper_41.txt 3\n",
      "train_data/james_madison/federalist_paper_37.txt 3\n",
      "train_data/james_madison/federalist_paper_38.txt 3\n",
      "train_data/james_madison/federalist_paper_44.txt 3\n",
      "train_data/james_madison/madison_state_of_unions_gutenberg_clean.txt 3\n",
      "train_data/james_madison/madison-the-writings-vol-2-1783-1787_cleaned.txt 3\n",
      "train_data/james_madison/madison_observations_draft_constitution_virginia_clean.txt 3\n",
      "train_data/james_madison/federalist_paper_43.txt 3\n",
      "train_data/james_madison/madison_helvidius_clean.txt 3\n",
      "train_data/james_madison/madison_misc_letters_addresses_clean.txt 3\n",
      "train_data/james_madison/madison_rose_negotiation_notes_clean.txt 3\n",
      "train_data/alexander_hamilton/federalist_paper_25.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_75.txt 4\n",
      "train_data/alexander_hamilton/hamilton_farmer_refuted_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_1.txt 4\n",
      "train_data/alexander_hamilton/hamilton_coinage_and_mint_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_7.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_73.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_36.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_79.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_74.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_72.txt 4\n",
      "train_data/alexander_hamilton/hamilton_explanation_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_81.txt 4\n",
      "train_data/alexander_hamilton/hamilton_the_continentalist_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_civis_to_mercator_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_33.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_66.txt 4\n",
      "train_data/alexander_hamilton/hamilton_constitution_amendments_draft_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_60.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_77.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_65.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_23.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_82.txt 4\n",
      "train_data/alexander_hamilton/hamilton_defense_funding_system_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_26.txt 4\n",
      "train_data/alexander_hamilton/hamilton_full_vindication_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_32.txt 4\n",
      "train_data/alexander_hamilton/hamilton_pacifus_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_vindication_of_funding_system_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_lucius_crassus_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_phocion_letters_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_35.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_24.txt 4\n",
      "train_data/alexander_hamilton/hamilton_misc_pseudonym_publications_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_27.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_85.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_59.txt 4\n",
      "train_data/alexander_hamilton/hamilton_horatius_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_34.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_9.txt 4\n",
      "train_data/alexander_hamilton/hamilton_croswell_case_speech_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_the_stand_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_8.txt 4\n",
      "train_data/alexander_hamilton/hamilton_the_warning_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_83.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_29.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_30.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_78.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_76.txt 4\n",
      "train_data/alexander_hamilton/hamilton_misc_letters_addresses_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_6.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_16.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_17.txt 4\n",
      "train_data/alexander_hamilton/hamilton_vindication_of_congress_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_67.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_31.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_21.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_11.txt 4\n",
      "train_data/alexander_hamilton/hamilton_convention_new_york_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_tully_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_national_bank_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_15.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_13.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_70.txt 4\n",
      "train_data/alexander_hamilton/hamilton_remarks_quebec_bill_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_28.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_12.txt 4\n",
      "train_data/alexander_hamilton/hamilton_camillus_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_80.txt 4\n",
      "train_data/alexander_hamilton/hamilton_no_jacobin_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_22.txt 4\n",
      "train_data/alexander_hamilton/hamilton_constitution_draft_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_publius_not_federalist_paper_clean.txt 4\n",
      "train_data/alexander_hamilton/hamilton_americanus_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_71.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_61.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_68.txt 4\n",
      "train_data/alexander_hamilton/hamilton_annapolis_convention_clean.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_69.txt 4\n",
      "train_data/alexander_hamilton/federalist_paper_84.txt 4\n",
      "train_data/james_monroe/monroe_state_of_the_unions_gutenberg_clean.txt 5\n",
      "train_data/john_jay/federalist_paper_5.txt 6\n",
      "train_data/john_jay/federalist_paper_2.txt 6\n",
      "train_data/john_jay/federalist_paper_3.txt 6\n",
      "train_data/john_jay/federalist_paper_4.txt 6\n",
      "train_data/john_jay/federalist_paper_64.txt 6\n",
      "train_data/george_washington/washington_state_of_unions_gutenberg_clean.txt 7\n",
      "train_data/george_washington/washington_misc_letters_addresses_clean.txt 7\n",
      "141378\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "vocab, X_train, y_train, author_to_id = load_train_data()\n",
    "num_classes = len(np.unique(y_train))\n",
    "print vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[[ 185    1 3753 ...,  359    5  495]]\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(vocab):\n",
    "    eval_data_dir = \"unknown_data\"\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        expanded_X = np.array(current)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = np.array([expanded_X])\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        eval_y[id] = np.array([author_to_id['james_madison']])\n",
    "\n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(vocab)\n",
    "print eval_y['18']\n",
    "print eval_X['18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 1 1 1 1 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 5 6 6 6 6 6 7 7 8]\n",
      "[array([   214,      3, 110275, ...,      3,    107, 117020])\n",
      " array([42483,   344, 22831, ...,     1,   135, 14900])\n",
      " array([   279, 116136,   3396, ...,   6758,      3,  21317])\n",
      " array([   47,   192,     3, ...,   591,   516, 38425])\n",
      " array([   13,    58,    19, ...,  2333,   120, 96859])\n",
      " array([61541,     1,   135, ...,   432,     6, 38425])\n",
      " array([   13,   789,    27, ...,     5,  3499, 45432])\n",
      " array([   11,    12,     8, ...,     5, 51014,  1028])\n",
      " array([ 56122,    132,   1821, ...,     14,    613, 118907])\n",
      " array([   13,   156,     8, ...,   169, 15270, 57131])\n",
      " array([  1028,      4,    712, ...,      3,      2, 125230])\n",
      " array([   13,   696,     4, ...,   527,     6, 27168])\n",
      " array([   13,    31,    18, ..., 77518,     5,  7580])\n",
      " array([   2,  132,  346, ...,    5,  231, 3420])\n",
      " array([  11,   31, 1149, ...,    5,  313, 3420])\n",
      " array([  2, 418, 271, ...,   3,   5, 690])\n",
      " array([ 157, 1149,    9, ...,    5,  278, 3420])\n",
      " array([ 157, 8665,    2, ...,    5,  346, 3420])\n",
      " array([   2,  418, 1129, ...,    5,  704, 3420])\n",
      " array([     7,    482,      3, ...,      3,    220, 124105])\n",
      " array([    13,    102,     21, ..., 114169,      5,  21061])\n",
      " array([   44,  1750,    22, ...,   182,   143, 51870])\n",
      " array([ 184,    2, 1018, ...,    5, 1707, 3420])\n",
      " array([7819,    2,  116, ...,    5,   43, 3420])\n",
      " array([  42,   19,  397, ...,    5,  715, 3420])\n",
      " array([   2,  137,  303, ...,  746, 3012, 3420])\n",
      " array([   7, 7549,    2, ...,  103,    5, 3294])\n",
      " array([  11,   12,   21, ...,    5, 1664, 3420])\n",
      " array([   8, 3018, 1129, ...,    5, 1879, 3420])\n",
      " array([  35,    2,  484, ...,   12,    5, 2446])\n",
      " array([ 613,    3,    2, ..., 6827,  516, 4965])\n",
      " array([ 22727,    714,      3, ...,      3, 121423,  66101])\n",
      " array([   2, 2076, 1129, ...,    5,   54, 3420])\n",
      " array([ 57471,    132,     31, ...,     20,     32, 141555])\n",
      " array([ 28329,    248,     17, ...,     18,      9, 122739])\n",
      " array([     1,   6717,    977, ...,   9650,      4, 121872])\n",
      " array([  11,   40,  249, ...,    5, 2906, 3420])\n",
      " array([   2,  208,   12, ...,    5,  627, 3420])\n",
      " array([19151, 34633,  3800, ...,   432,     4, 21317])\n",
      " array([ 171,   36, 3623, ...,  103,    5,  137])\n",
      " array([  1886,      4,  42536, ...,    301,      3, 123882])\n",
      " array([   11,    12,   758, ...,  1356,     5, 14355])\n",
      " array([   2,  671, 4074, ...,   24,    5,  276])\n",
      " array([  42,   19,  397, ...,  229,    5, 7919])\n",
      " array([ 394,    4, 6621, ...,  239,    5,  186])\n",
      " array([   2,  208,    3, ...,    5, 3472, 3420])\n",
      " array([   2,  510,    3, ...,    5, 2022, 3420])\n",
      " array([  1264,      1,    186, ..., 116951,   1886,  23764])\n",
      " array([306,  72,  95, ..., 394,   5, 346])\n",
      " array([    5,    50, 84966, ...,     3,  4962, 74944])\n",
      " array([  1320,      1,    135, ...,     39,     37, 135205])\n",
      " array([   2, 2502,    3, ...,    5,  278, 3420])\n",
      " array([   8, 1810,    3, ...,    3,    5, 5378])\n",
      " array([    1,    42,     2, ...,    53,     3, 25775])\n",
      " array([42, 19,  1, ..., 24,  5, 75])\n",
      " array([  11,   39,   37, ...,  229,    5, 7919])\n",
      " array([   2, 1189,  185, ...,    5, 2839, 3420])\n",
      " array([   2,  402,    3, ...,    5, 1528, 3420])\n",
      " array([   2, 6091,    3, ..., 1917,    5,  135])\n",
      " array([  816,     3,     2, ...,    48,    53, 93581])\n",
      " array([  11,   31,    8, ...,    5, 3458, 3420])\n",
      " array([   361,      6, 111267, ...,   1238,      6,  74759])\n",
      " array([ 822,   13,  102, ...,    5,  442, 3420])\n",
      " array([    5,    50, 95938, ...,  4231,     8, 53971])\n",
      " array([  5751,      3,      2, ..., 125721, 118755, 123349])\n",
      " array([ 1273,     1,   186, ...,    28, 13288, 23764])\n",
      " array([  270,    21,    78, ...,    99,  1405, 46169])\n",
      " array([ 127,   42,  619, ...,    5,  205, 3420])\n",
      " array([  4,   2, 185, ...,  24,   5, 169])\n",
      " array([ 73759, 138934,    215, ...,     12,     21,  63520])\n",
      " array([  11,   39,   37, ...,  779,    5, 8489])\n",
      " array([  388,     4,     2, ..., 29405, 38987,     2])\n",
      " array([  2, 468, 230, ..., 977,   5, 239])\n",
      " array([ 2058,     6,  1551, ...,    18,    28, 96168])\n",
      " array([  13, 2520,  199, ...,    5,  378, 3420])\n",
      " array([   8, 1591,  278, ..., 8490,    5,   13])\n",
      " array([   40,    11,  1360, ...,    25,    10, 76328])\n",
      " array([     2,    752,    135, ..., 123599,  14851,  23764])\n",
      " array([4817,   11,  138, ...,   24,    5,  116])\n",
      " array([     2,   3559, 119806, ...,     29,      8,  45279])\n",
      " array([   2,  621,    4, ...,    6,    5, 9790])\n",
      " array([   2,   97,    3, ...,    5, 3763, 3420])\n",
      " array([  11,   39,   37, ...,    5,   11, 3420])\n",
      " array([  42,  619,   95, ..., 1719,    5, 3416])\n",
      " array([    2,   208,    12, ...,     7, 29356,  3420])\n",
      " array([ 73127,      4,     47, ...,      7,     41, 142475])\n",
      " array([    2,   237,   132, ...,   317,     5, 39598])\n",
      " array([    2,  1074,     3, ...,     5, 18185,  3420])\n",
      " array([  36,    1,  621, ...,    5,  395, 3420])\n",
      " array([  145,   150,  4585, ...,     3,     2, 91446])\n",
      " array([  2, 137,   3, ..., 963,   5, 135])\n",
      " array([    7, 10615,     3, ...,     5,    57,  3420])\n",
      " array([ 157,    7,    2, ...,    5,  443, 3420])\n",
      " array([    2,   494,     3, ...,  8388,  4446, 61230])\n",
      " array([     5,     73, 115293, ...,    976,     12,  83049])\n",
      " array([81572,     1,   186, ...,   537,     3, 79784])\n",
      " array([1886,    4, 5115, ...,    5, 1029, 4031])\n",
      " array([    7,     2,   168, ...,    18,     2, 16539])\n",
      " array([  14, 1284,   20, ...,    5,  107, 3420])\n",
      " array([  59,   12,   36, ...,  229,    5, 7919])\n",
      " array([     5,     50, 105165, ...,      6,   4386,  44155])\n",
      " array([   9,   59,   40, ..., 2045,    5, 1375])\n",
      " array([   2,  921,    3, ...,  604,    5, 1338])\n",
      " array([    15,  16399,    135, ...,   9309,      8, 118249])\n",
      " array([   4,  438,   20, ...,    5,  663, 3420])\n",
      " array([    50, 109235,      2, ...,   3443, 106208,  61818])\n",
      " array([  7, 956,   4, ...,   8,   5, 456])\n",
      " array([    2,   123,     3, ...,  3389,     2, 78497])\n",
      " array([   59,    30,  2043, ...,     3,  9325, 42863])\n",
      " array([119574,      1,    135, ...,     36,   3365,  55076])\n",
      " array([1740,    7,  324, ...,    2,    5,  123])\n",
      " array([   2,   56, 2100, ...,    5, 7148, 3420])\n",
      " array([     2,    533,      3, ...,  90744,      5, 113690])\n",
      " array([    4,     2,  1565, ...,     2,    54, 97797])\n",
      " array([  13,  619,   95, ...,   93,    5, 1317])\n",
      " array([    7,     2,   168, ...,     6,     5, 15136])\n",
      " array([   35,    50,   484, ...,     3,     5, 15139])\n",
      " array([ 3942,     1, 16424, ...,     5,   148,  3420])\n",
      " array([    98,      2,    123, ...,     46, 121152,   3420])\n",
      " array([  11,   12,   21, ...,  105,  139, 3420])\n",
      " array([  46,  132,  346, ...,    5,  202, 3420])\n",
      " array([  11,   12,    8, ...,    5, 5378, 3420])\n",
      " array([    13,   2354,     20, ...,      5, 104911,    712])\n",
      " array([  672,     1,    15, ...,     1,     3, 47280])\n",
      " array([    1, 72603,    35, ..., 66877,     5, 53167])]\n"
     ]
    }
   ],
   "source": [
    "print y_train\n",
    "print X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(publications, authors, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form.\"\"\"\n",
    "    for i, ids in enumerate(publications):\n",
    "        # Clip to multiple of max_time for convenience\n",
    "        clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
    "        \n",
    "        input_w = ids[:clip_len]     # current word\n",
    "        target_y = ids[1:clip_len+1]  # next word\n",
    "        # Reshape so we can select columns\n",
    "        input_w = input_w.reshape([batch_size,-1])\n",
    "\n",
    "        # Yield batches\n",
    "        for j in xrange(0, input_w.shape[1], max_time):\n",
    "            this_w = input_w[:,j:j+max_time]\n",
    "            yield this_w, np.full_like(this_w, authors[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[   214      3 110275     36    262]\n",
      " [    30  38231      1  15231    243]\n",
      " [     1    348      6  41141      6]\n",
      " [   423      1   5339     52      3]\n",
      " [  1719    132   1606      7      2]\n",
      " [  5258    393      2      1   4648]\n",
      " [   522      2  97611    423    508]\n",
      " [  3039      6      1   1505      3]\n",
      " [    17  96856      3  77690      2]\n",
      " [    78      8    401 126840      1]]\n"
     ]
    }
   ],
   "source": [
    "for i, (w, y) in enumerate(batch_generator(X_train, y_train, 10, 5)):\n",
    "    print y\n",
    "    print w\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y,\n",
    "                     lm.initial_h_: h,\n",
    "                     lm.learning_rate_: learning_rate,\n",
    "                     lm.use_dropout_: use_dropout}\n",
    "        \n",
    "        _, h, cost = session.run([train_op, lm.final_h_, lm.loss_], feed_dict)  \n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, authors, max_time, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = batch_generator(ids, authors, batch_size=100, max_time=max_time)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_paper(lm, session, batch_iterator):\n",
    "    for i, (w, y) in enumerate(batch_iterator):        \n",
    "        feed_dict = {lm.input_w_: w,\n",
    "                     lm.target_y_: y}\n",
    "        \n",
    "        cost, truths, logits, predictions = session.run([lm.loss_, lm.target_y_last_, lm.logits_last_, lm.predictions_], feed_dict)  \n",
    "        print \"predictions:\", predictions.reshape(-1)\n",
    "        print \"truths:\", truths.reshape(-1)\n",
    "        print \"logits:\", logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 15\n",
    "batch_size = 40\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    num_classes=num_classes,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 3866]: seen 2310600 words at 23102 wps, loss = 0.015\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildClassifierGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(X_train, y_train, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        # Run a training epoch.\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=100, learning_rate=learning_rate)\n",
    "    \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, eval_X['18'], eval_y['18'], max_time, name=\"Federalist Paper 18\")\n",
    "        #score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "\n",
    "    bi = batch_generator(eval_X['18'], eval_y['18'], batch_size, max_time)\n",
    "    predict_paper(lm, session, bi)\n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "{'thomas_jefferson': 1, 'john_adams': 2, 'alexander_hamilton': 4, 'benjamin_franklin': 8, 'george_washington': 7, 'thomas_paine': 0, 'james_madison': 3, 'james_monroe': 5, 'john_jay': 6}\n"
     ]
    }
   ],
   "source": [
    "print author_to_id['james_madison']\n",
    "print author_to_id['george_washington']\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
