{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf  #TF 1.1.0rc1\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import matplotlib.pyplot as plt\n",
    "from tsc_model import Model,sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'I', 'am'],\n",
       " ['I', 'am', 'mr'],\n",
       " ['am', 'mr', '.'],\n",
       " ['mr', '.', 'anderson'],\n",
       " ['.', 'anderson', 'what'],\n",
       " ['anderson', 'what', 'is'],\n",
       " ['what', 'is', 'your'],\n",
       " ['is', 'your', 'name'],\n",
       " ['your', 'name', '?']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_up_words(words, window_size=10):\n",
    "    slices = []\n",
    "    for index, word in enumerate(words):\n",
    "        slice = words[index:index+window_size]\n",
    "        if len(slice) == window_size:\n",
    "            slices.append(slice)\n",
    "        else:\n",
    "            break\n",
    "    return slices\n",
    "        \n",
    "\n",
    "slice_up_words([\"hello\", \"I\", \"am\", \"mr\", \".\", \"anderson\", \"what\", \"is\", \"your\", \"name\", \"?\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def load_train_data(window_size):\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                content = f.read()\n",
    "                current = []\n",
    "                for word in content.split(\" \"):   \n",
    "                    if word[-1] in (\".\", ',', '?', ';', '!'):\n",
    "                        punk = word[-1]\n",
    "                        all_tokens.append(punk)\n",
    "                        current.append(punk)\n",
    "                        word = word[0:-1]\n",
    "                        \n",
    "                    word = canonicalize_word(word)\n",
    "                    all_tokens.append(word)\n",
    "                    current.append(word)\n",
    "                X.append(current)\n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = vocab.words_to_ids(x)\n",
    "\n",
    "    expanded_y = np.array([])\n",
    "    expanded_X = None\n",
    "    for i, x in enumerate(X):\n",
    "        slices = slice_up_words(x, window_size)\n",
    "        if expanded_X is None:\n",
    "            expanded_X = np.array(slices)\n",
    "        else:\n",
    "            expanded_X = np.append(expanded_X, np.array(slices), axis=0)\n",
    "        expanded_y = np.append(expanded_y, np.array([y[i]] * len(slices)))\n",
    "\n",
    "    return vocab, expanded_X, expanded_y\n",
    "\n",
    "num_words_per_x = 15\n",
    "vocab, X_train, y_train = load_train_data(num_words_per_x)\n",
    "N,sl = X_train.shape\n",
    "num_classes = len(np.unique(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set these directories\n",
    "#direc = './data'\n",
    "#summaries_dir = '.'\n",
    "\n",
    "\"\"\"Load the data\"\"\"\n",
    "#ratio = np.array([0.8,0.9]) #Ratios where to split the training and validation set\n",
    "#X_train,X_val,X_test,y_train,y_val,y_test = load_data(direc,ratio,dataset='ChlorineConcentration')\n",
    "#N,sl = X_train.shape\n",
    "#num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Hyperparamaters\"\"\"\n",
    "num_epochs = 2\n",
    "batch_size = 50\n",
    "dropout = 0.8\n",
    "config = {    'num_layers' :    3,               #number of layers of stacked RNN's\n",
    "              'hidden_size' :   120,             #memory cells in a layer\n",
    "              'max_grad_norm' : 5,             #maximum gradient norm during training\n",
    "              'batch_size' :    batch_size,\n",
    "              'learning_rate' : .005,\n",
    "              'sl':             sl,\n",
    "              'num_classes':    num_classes}\n",
    "\n",
    "max_iterations = int(np.floor((num_epochs * N) / batch_size) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 176004 samples in approximately 2 epochs in 7041 iterations\n"
     ]
    }
   ],
   "source": [
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train %.0f samples in approximately %d epochs in %d iterations' %(N,epochs, max_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computation graph\n"
     ]
    }
   ],
   "source": [
    "#Instantiate a model\n",
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.755934\n",
      "0.72\n",
      "101\n",
      "0.920956\n",
      "0.6\n",
      "201\n",
      "0.668028\n",
      "0.72\n",
      "301\n",
      "0.717797\n",
      "0.76\n",
      "401\n",
      "0.720618\n",
      "0.74\n",
      "501\n",
      "0.622645\n",
      "0.76\n",
      "601\n",
      "0.698615\n",
      "0.78\n",
      "701\n",
      "0.642542\n",
      "0.76\n",
      "801\n",
      "0.809578\n",
      "0.62\n",
      "901\n",
      "0.894874\n",
      "0.64\n",
      "1001\n",
      "0.714788\n",
      "0.68\n",
      "1101\n",
      "0.697955\n",
      "0.76\n",
      "1201\n",
      "0.795292\n",
      "0.64\n",
      "1301\n",
      "0.812237\n",
      "0.68\n",
      "1401\n",
      "0.973783\n",
      "0.64\n",
      "1501\n",
      "0.799212\n",
      "0.64\n",
      "1601\n",
      "0.839658\n",
      "0.6\n",
      "1701\n",
      "0.731584\n",
      "0.76\n",
      "1801\n",
      "0.655206\n",
      "0.7\n",
      "1901\n",
      "0.897379\n",
      "0.66\n",
      "2001\n",
      "0.786713\n",
      "0.68\n",
      "2101\n",
      "0.754485\n",
      "0.7\n",
      "2201\n",
      "0.522647\n",
      "0.84\n",
      "2301\n",
      "0.756294\n",
      "0.72\n",
      "2401\n",
      "0.799331\n",
      "0.62\n",
      "2501\n",
      "0.816572\n",
      "0.64\n",
      "2601\n",
      "0.828382\n",
      "0.68\n",
      "2701\n",
      "0.634791\n",
      "0.74\n",
      "2801\n",
      "0.7755\n",
      "0.7\n",
      "2901\n",
      "0.711901\n",
      "0.72\n",
      "3001\n",
      "0.760971\n",
      "0.7\n",
      "3101\n",
      "0.734863\n",
      "0.74\n",
      "3201\n",
      "0.780146\n",
      "0.68\n",
      "3301\n",
      "0.794957\n",
      "0.62\n",
      "3401\n",
      "0.661646\n",
      "0.72\n",
      "3501\n",
      "0.732163\n",
      "0.7\n",
      "3601\n",
      "0.830321\n",
      "0.62\n",
      "3701\n",
      "0.612894\n",
      "0.74\n",
      "3801\n",
      "0.818073\n",
      "0.62\n",
      "3901\n",
      "0.812762\n",
      "0.64\n",
      "4001\n",
      "0.619059\n",
      "0.78\n",
      "4101\n",
      "0.76134\n",
      "0.66\n",
      "4201\n",
      "0.711861\n",
      "0.68\n",
      "4301\n",
      "0.684261\n",
      "0.76\n",
      "4401\n",
      "0.832739\n",
      "0.66\n",
      "4501\n",
      "0.61486\n",
      "0.82\n",
      "4601\n",
      "0.841107\n",
      "0.66\n",
      "4701\n",
      "0.573759\n",
      "0.8\n",
      "4801\n",
      "0.767972\n",
      "0.64\n",
      "4901\n",
      "0.894589\n",
      "0.64\n",
      "5001\n",
      "0.730092\n",
      "0.7\n",
      "5101\n",
      "0.655538\n",
      "0.76\n",
      "5201\n",
      "0.638532\n",
      "0.76\n",
      "5301\n",
      "0.539794\n",
      "0.82\n",
      "5401\n",
      "0.931427\n",
      "0.64\n",
      "5501\n",
      "0.688074\n",
      "0.74\n",
      "5601\n",
      "0.703528\n",
      "0.7\n",
      "5701\n",
      "0.702431\n",
      "0.68\n",
      "5801\n",
      "0.737914\n",
      "0.7\n",
      "5901\n",
      "0.588449\n",
      "0.78\n",
      "6001\n",
      "0.705278\n",
      "0.7\n",
      "6101\n",
      "0.765663\n",
      "0.66\n",
      "6201\n",
      "0.79413\n",
      "0.76\n",
      "6301\n",
      "0.597598\n",
      "0.8\n",
      "6401\n",
      "0.802485\n",
      "0.64\n",
      "6501\n",
      "0.770025\n",
      "0.74\n",
      "6601\n",
      "0.624046\n",
      "0.74\n",
      "6701\n",
      "0.628304\n",
      "0.8\n",
      "6801\n",
      "0.844355\n",
      "0.6\n",
      "6901\n",
      "0.771553\n",
      "0.64\n",
      "7001\n",
      "0.762544\n",
      "0.7\n",
      "Trained 2.0 epochs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Session time\"\"\"\n",
    "sess = tf.Session() #Depending on your use, do not forget to close the session\n",
    "#writer = tf.summary.FileWriter(summaries_dir, sess.graph)  #writer for Tensorboard\n",
    "sess.run(model.init_op)\n",
    "\n",
    "cost_train_ma = -np.log(1/float(num_classes)+1e-9)  #Moving average training cost\n",
    "acc_train_ma = 0.0\n",
    "try:\n",
    "    for i in range(max_iterations):\n",
    "        X_batch, y_batch = sample_batch(X_train,y_train,batch_size)\n",
    "        #Next line does the actual training\n",
    "        cost_train, acc_train,_ = sess.run([model.cost,model.accuracy, model.train_op],feed_dict = {model.input: X_batch,model.labels: y_batch,model.keep_prob:dropout})\n",
    "        if i%100 == 1:\n",
    "            print i\n",
    "            print cost_train\n",
    "            print acc_train\n",
    "        cost_train_ma = cost_train_ma*0.99 + cost_train*0.01\n",
    "        acc_train_ma = acc_train_ma*0.99 + acc_train*0.01\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "  pass\n",
    "  \n",
    "epoch = float(i)*batch_size/N\n",
    "#print('Trained %.1f epochs, accuracy is %5.3f and cost is %5.3f'%(epoch,acc_val,cost_val))\n",
    "print('Trained %.1f epochs'%(epoch))\n",
    "\n",
    "#now run in your terminal:\n",
    "# $ tensorboard --logdir = <summaries_dir>\n",
    "# Replace <summaries_dir> with your own dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if i%100 == 1:\n",
    "    #Evaluate validation performance\n",
    "      X_batch, y_batch = sample_batch(X_val,y_val,batch_size)\n",
    "      cost_val, summ, acc_val = sess.run([model.cost,model.merged,model.accuracy],feed_dict = {model.input: X_batch, model.labels: y_batch, model.keep_prob:1.0})\n",
    "      print('At %5.0f/%5.0f: COST %5.3f/%5.3f(%5.3f) -- Acc %5.3f/%5.3f(%5.3f)' %(i,max_iterations,cost_train,cost_val,cost_train_ma,acc_train,acc_val,acc_train_ma))\n",
    "      #Write information to TensorBoard\n",
    "      writer.add_summary(summ, i)\n",
    "      writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
