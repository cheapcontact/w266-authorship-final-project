{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf  #TF 1.1.0rc1\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import matplotlib.pyplot as plt\n",
    "from tsc_model import Model,sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'I', 'am'], ['mr', '.', 'anderson'], ['what', 'is', 'your']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_up_words(words, window_size=10, step_size=1):\n",
    "    slices = []\n",
    "    num_words = len(words)\n",
    "    for index in range(0, num_words, step_size):\n",
    "        slice = words[index:index+window_size]\n",
    "        if len(slice) == window_size:\n",
    "            slices.append(slice)\n",
    "        else:\n",
    "            break\n",
    "    return slices\n",
    "        \n",
    "\n",
    "slice_up_words([\"hello\", \"I\", \"am\", \"mr\", \".\", \"anderson\", \"what\", \"is\", \"your\", \"name\", \"?\"], 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    for word in words.split(\" \"):   \n",
    "        if word[-1] in (\".\", ',', '?', ';', '!'):\n",
    "            punk = word[-1]\n",
    "            current.append(punk)\n",
    "            word = word[0:-1]\n",
    "\n",
    "        word = canonicalize_word(word)\n",
    "        current.append(word)\n",
    "    return current\n",
    "      \n",
    "def load_train_data(window_size, step_size):\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(current)\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = vocab.words_to_ids(x)\n",
    "\n",
    "    expanded_y = np.array([])\n",
    "    expanded_X = None\n",
    "    for i, x in enumerate(X):\n",
    "        slices = slice_up_words(x, window_size, step_size)\n",
    "        if expanded_X is None:\n",
    "            expanded_X = np.array(slices)\n",
    "        else:\n",
    "            expanded_X = np.append(expanded_X, np.array(slices), axis=0)\n",
    "        expanded_y = np.append(expanded_y, np.array([y[i]] * len(slices)))\n",
    "\n",
    "    return vocab, expanded_X, expanded_y, author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33323\n",
      "{'thomas_jefferson': 0, 'john_adams': 1, 'alexander_hamilton': 3, 'george_washington': 6, 'james_madison': 2, 'james_monroe': 4, 'john_jay': 5}\n"
     ]
    }
   ],
   "source": [
    "num_words_per_x = 15\n",
    "vocab, X_train, y_train, author_to_id = load_train_data(num_words_per_x, num_words_per_x)\n",
    "N,sl = X_train.shape\n",
    "num_classes = len(np.unique(y_train))\n",
    "print vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '58': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '49': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2]), '55': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '54': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2]), '57': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '56': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2]), '51': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2]), '50': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '53': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '52': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '19': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '62': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '63': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), '18': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(window_size, step_size, vocab):\n",
    "    eval_data_dir = \"unknown_data\"\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        slices = slice_up_words(current, window_size, step_size)\n",
    "        expanded_X = np.array(slices)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = expanded_X\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        eval_y[id] = np.array([author_to_id['james_madison']] * len(slices))\n",
    "\n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(num_words_per_x, num_words_per_x, vocab)\n",
    "print eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set these directories\n",
    "#direc = './data'\n",
    "#summaries_dir = '.'\n",
    "\n",
    "\"\"\"Load the data\"\"\"\n",
    "#ratio = np.array([0.8,0.9]) #Ratios where to split the training and validation set\n",
    "#X_train,X_val,X_test,y_train,y_val,y_test = load_data(direc,ratio,dataset='ChlorineConcentration')\n",
    "#N,sl = X_train.shape\n",
    "#num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Hyperparamaters\"\"\"\n",
    "num_epochs = 1\n",
    "batch_size = 50\n",
    "dropout = 0.8\n",
    "config = {    'num_layers' :    3,               #number of layers of stacked RNN's\n",
    "              'hidden_size' :   120,             #memory cells in a layer\n",
    "              'max_grad_norm' : 5,             #maximum gradient norm during training\n",
    "              'batch_size' :    batch_size,\n",
    "              'learning_rate' : .005,\n",
    "              'sl':             sl,\n",
    "              'num_classes':    num_classes}\n",
    "\n",
    "max_iterations = int(np.floor((num_epochs * N) / batch_size) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 64054 samples in approximately 1 epochs in 1282 iterations\n"
     ]
    }
   ],
   "source": [
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train %.0f samples in approximately %d epochs in %d iterations' %(N,epochs, max_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computation graph\n"
     ]
    }
   ],
   "source": [
    "#Instantiate a model\n",
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 group trained in 13 seconds\n",
      "Predicted Full Train Time: 288 minutes\n",
      "Predicted Full Train Time: 4.807146 hours\n",
      "200 group trained in 13 seconds\n",
      "300 group trained in 13 seconds\n",
      "400 group trained in 13 seconds\n",
      "500 group trained in 13 seconds\n",
      "600 group trained in 13 seconds\n",
      "Trained 0.5 epochs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"Session time\"\"\"\n",
    "sess = tf.Session() #Depending on your use, do not forget to close the session\n",
    "#writer = tf.summary.FileWriter(summaries_dir, sess.graph)  #writer for Tensorboard\n",
    "sess.run(model.init_op)\n",
    "\n",
    "cost_train_ma = -np.log(1/float(num_classes)+1e-9)  #Moving average training cost\n",
    "acc_train_ma = 0.0\n",
    "try:\n",
    "    start = time.clock()\n",
    "    for i in range(max_iterations):\n",
    "        X_batch, y_batch = sample_batch(X_train,y_train,batch_size)\n",
    "        #Next line does the actual training\n",
    "        cost_train, acc_train,_ = sess.run([model.cost,model.accuracy, model.train_op],feed_dict = {model.input: X_batch,model.labels: y_batch,model.keep_prob:dropout})\n",
    "        if i != 0 and i%100 == 0:\n",
    "            elapsed_time = time.clock() - start\n",
    "            print \"%d group trained in %d seconds\" % (i, elapsed_time)\n",
    "            start = time.clock()\n",
    "            if i == 100:\n",
    "                print \"Predicted Full Train Time: %d minutes\" % (max_iterations * elapsed_time / 60)\n",
    "                print \"Predicted Full Train Time: %f hours\" % (max_iterations * elapsed_time / (60 * 60))\n",
    "\n",
    "        cost_train_ma = cost_train_ma*0.99 + cost_train*0.01\n",
    "        acc_train_ma = acc_train_ma*0.99 + acc_train*0.01\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "  pass\n",
    "  \n",
    "epoch = float(i)*batch_size/N\n",
    "#print('Trained %.1f epochs, accuracy is %5.3f and cost is %5.3f'%(epoch,acc_val,cost_val))\n",
    "print('Trained %.1f epochs'%(epoch))\n",
    "\n",
    "#now run in your terminal:\n",
    "# $ tensorboard --logdir = <summaries_dir>\n",
    "# Replace <summaries_dir> with your own dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating federalist paper #20\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #58\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #49\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #55\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #54\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #57\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #56\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #51\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #50\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #53\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #52\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #19\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #62\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #63\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #18\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluate validation performance\n",
    "# This is classifying batchs from disputed paper #20\n",
    "\n",
    "total_acc = 0\n",
    "for key, value in eval_X.iteritems():\n",
    "    print \"Evaluating federalist paper #%s\" % key\n",
    "    for i in range(1000):\n",
    "        X_batch, y_batch = sample_batch(value,eval_y[key],batch_size)\n",
    "        predictions, acc_val = sess.run([model.predictions, model.accuracy],feed_dict = {model.input: X_batch, model.labels: y_batch, model.keep_prob:1.0})\n",
    "    #    print \"Predictions:\", predictions\n",
    "    #    print \"Suspected Actual: \", y_batch\n",
    "    #    print \"accuracy: %5.3f\" % acc_val\n",
    "        total_acc += acc_val\n",
    "        if i != 0 and i % 100 == 0:\n",
    "            print \"Accuracy at iteration %d: %.2f\" % (i, total_acc/i)\n",
    "print total_acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
