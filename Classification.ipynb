{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf  #TF 1.1.0rc1\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import matplotlib.pyplot as plt\n",
    "from tsc_model import Model,sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'I', 'am'],\n",
       " ['I', 'am', 'mr'],\n",
       " ['am', 'mr', '.'],\n",
       " ['mr', '.', 'anderson'],\n",
       " ['.', 'anderson', 'what'],\n",
       " ['anderson', 'what', 'is'],\n",
       " ['what', 'is', 'your'],\n",
       " ['is', 'your', 'name'],\n",
       " ['your', 'name', '?']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_up_words(words, window_size=10):\n",
    "    slices = []\n",
    "    for index, word in enumerate(words):\n",
    "        slice = words[index:index+window_size]\n",
    "        if len(slice) == window_size:\n",
    "            slices.append(slice)\n",
    "        else:\n",
    "            break\n",
    "    return slices\n",
    "        \n",
    "\n",
    "slice_up_words([\"hello\", \"I\", \"am\", \"mr\", \".\", \"anderson\", \"what\", \"is\", \"your\", \"name\", \"?\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    for word in words.split(\" \"):   \n",
    "        if word[-1] in (\".\", ',', '?', ';', '!'):\n",
    "            punk = word[-1]\n",
    "            current.append(punk)\n",
    "            word = word[0:-1]\n",
    "\n",
    "        word = canonicalize_word(word)\n",
    "        current.append(word)\n",
    "    return current\n",
    "      \n",
    "def load_train_data(window_size):\n",
    "    train_data_dir = 'train_data'\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        for file_name in listdir(author_path):\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(current)\n",
    "                \n",
    "    vocab = Vocabulary(all_tokens)\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        X[i] = vocab.words_to_ids(x)\n",
    "\n",
    "    expanded_y = np.array([])\n",
    "    expanded_X = None\n",
    "    for i, x in enumerate(X):\n",
    "        slices = slice_up_words(x, window_size)\n",
    "        if expanded_X is None:\n",
    "            expanded_X = np.array(slices)\n",
    "        else:\n",
    "            expanded_X = np.append(expanded_X, np.array(slices), axis=0)\n",
    "        expanded_y = np.append(expanded_y, np.array([y[i]] * len(slices)))\n",
    "\n",
    "    return vocab, expanded_X, expanded_y, author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8626\n",
      "{'james_madison': 0, 'john_jay': 2, 'alexander_hamilton': 1}\n"
     ]
    }
   ],
   "source": [
    "num_words_per_x = 15\n",
    "vocab, X_train, y_train, author_to_id = load_train_data(num_words_per_x)\n",
    "N,sl = X_train.shape\n",
    "num_classes = len(np.unique(y_train))\n",
    "print vocab.size\n",
    "print author_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20': array([0, 0, 0, ..., 0, 0, 0]), '58': array([0, 0, 0, ..., 0, 0, 0]), '49': array([0, 0, 0, ..., 0, 0, 0]), '55': array([0, 0, 0, ..., 0, 0, 0]), '54': array([0, 0, 0, ..., 0, 0, 0]), '57': array([0, 0, 0, ..., 0, 0, 0]), '56': array([0, 0, 0, ..., 0, 0, 0]), '51': array([0, 0, 0, ..., 0, 0, 0]), '50': array([0, 0, 0, ..., 0, 0, 0]), '53': array([0, 0, 0, ..., 0, 0, 0]), '52': array([0, 0, 0, ..., 0, 0, 0]), '19': array([0, 0, 0, ..., 0, 0, 0]), '62': array([0, 0, 0, ..., 0, 0, 0]), '63': array([0, 0, 0, ..., 0, 0, 0]), '18': array([0, 0, 0, ..., 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "def load_eval_data(window_size, vocab):\n",
    "    eval_data_dir = \"unknown_data\"\n",
    "    eval_X = {}\n",
    "    eval_y = {}\n",
    "    for file_name in listdir(eval_data_dir):\n",
    "        full_path = \"%s/%s\" % (eval_data_dir, file_name)\n",
    "        with open(full_path, \"r\") as f:\n",
    "            current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "\n",
    "        slices = slice_up_words(current, window_size)\n",
    "        expanded_X = np.array(slices)\n",
    "        id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "        eval_X[id] = expanded_X\n",
    "        # working with the assumption that James Madison wrote all the disputed papers\n",
    "        eval_y[id] = np.array([author_to_id['james_madison']] * len(slices))\n",
    "\n",
    "    return eval_X, eval_y\n",
    "\n",
    "eval_X, eval_y = load_eval_data(num_words_per_x, vocab)\n",
    "print eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set these directories\n",
    "#direc = './data'\n",
    "#summaries_dir = '.'\n",
    "\n",
    "\"\"\"Load the data\"\"\"\n",
    "#ratio = np.array([0.8,0.9]) #Ratios where to split the training and validation set\n",
    "#X_train,X_val,X_test,y_train,y_val,y_test = load_data(direc,ratio,dataset='ChlorineConcentration')\n",
    "#N,sl = X_train.shape\n",
    "#num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Hyperparamaters\"\"\"\n",
    "num_epochs = 1\n",
    "batch_size = 50\n",
    "dropout = 0.8\n",
    "config = {    'num_layers' :    3,               #number of layers of stacked RNN's\n",
    "              'hidden_size' :   120,             #memory cells in a layer\n",
    "              'max_grad_norm' : 5,             #maximum gradient norm during training\n",
    "              'batch_size' :    batch_size,\n",
    "              'learning_rate' : .005,\n",
    "              'sl':             sl,\n",
    "              'num_classes':    num_classes}\n",
    "\n",
    "max_iterations = int(np.floor((num_epochs * N) / batch_size) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 176004 samples in approximately 0 epochs in 353 iterations\n"
     ]
    }
   ],
   "source": [
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train %.0f samples in approximately %d epochs in %d iterations' %(N,epochs, max_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computation graph\n"
     ]
    }
   ],
   "source": [
    "#Instantiate a model\n",
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.32169\n",
      "0.62\n",
      "101\n",
      "0.7466\n",
      "0.74\n",
      "201\n",
      "0.782717\n",
      "0.66\n",
      "301\n",
      "0.65979\n",
      "0.8\n",
      "401\n",
      "0.602412\n",
      "0.78\n",
      "501\n",
      "0.887314\n",
      "0.6\n",
      "601\n",
      "0.689751\n",
      "0.74\n",
      "701\n",
      "0.696101\n",
      "0.78\n",
      "801\n",
      "0.882722\n",
      "0.62\n",
      "901\n",
      "0.765375\n",
      "0.68\n",
      "1001\n",
      "0.77599\n",
      "0.66\n",
      "1101\n",
      "0.99102\n",
      "0.56\n",
      "1201\n",
      "0.775643\n",
      "0.66\n",
      "1301\n",
      "0.710101\n",
      "0.76\n",
      "1401\n",
      "0.72777\n",
      "0.66\n",
      "1501\n",
      "0.608596\n",
      "0.78\n",
      "1601\n",
      "0.924865\n",
      "0.62\n",
      "1701\n",
      "0.833966\n",
      "0.66\n",
      "1801\n",
      "0.612032\n",
      "0.76\n",
      "1901\n",
      "0.698124\n",
      "0.74\n",
      "2001\n",
      "0.68245\n",
      "0.76\n",
      "2101\n",
      "0.724238\n",
      "0.7\n",
      "2201\n",
      "0.895442\n",
      "0.58\n",
      "2301\n",
      "0.634508\n",
      "0.76\n",
      "2401\n",
      "0.78423\n",
      "0.68\n",
      "2501\n",
      "0.634059\n",
      "0.76\n",
      "2601\n",
      "0.923063\n",
      "0.6\n",
      "2701\n",
      "0.796587\n",
      "0.64\n",
      "2801\n",
      "0.866879\n",
      "0.66\n",
      "2901\n",
      "0.603574\n",
      "0.82\n",
      "3001\n",
      "1.02338\n",
      "0.62\n",
      "3101\n",
      "0.768898\n",
      "0.66\n",
      "3201\n",
      "0.792293\n",
      "0.66\n",
      "3301\n",
      "0.900301\n",
      "0.58\n",
      "3401\n",
      "0.607615\n",
      "0.78\n",
      "3501\n",
      "0.628267\n",
      "0.78\n",
      "Trained 1.0 epochs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Session time\"\"\"\n",
    "sess = tf.Session() #Depending on your use, do not forget to close the session\n",
    "#writer = tf.summary.FileWriter(summaries_dir, sess.graph)  #writer for Tensorboard\n",
    "sess.run(model.init_op)\n",
    "\n",
    "cost_train_ma = -np.log(1/float(num_classes)+1e-9)  #Moving average training cost\n",
    "acc_train_ma = 0.0\n",
    "try:\n",
    "    for i in range(max_iterations):\n",
    "        X_batch, y_batch = sample_batch(X_train,y_train,batch_size)\n",
    "        #Next line does the actual training\n",
    "        cost_train, acc_train,_ = sess.run([model.cost,model.accuracy, model.train_op],feed_dict = {model.input: X_batch,model.labels: y_batch,model.keep_prob:dropout})\n",
    "        if i%100 == 1:\n",
    "            print i\n",
    "            print cost_train\n",
    "            print acc_train\n",
    "        cost_train_ma = cost_train_ma*0.99 + cost_train*0.01\n",
    "        acc_train_ma = acc_train_ma*0.99 + acc_train*0.01\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "  pass\n",
    "  \n",
    "epoch = float(i)*batch_size/N\n",
    "#print('Trained %.1f epochs, accuracy is %5.3f and cost is %5.3f'%(epoch,acc_val,cost_val))\n",
    "print('Trained %.1f epochs'%(epoch))\n",
    "\n",
    "#now run in your terminal:\n",
    "# $ tensorboard --logdir = <summaries_dir>\n",
    "# Replace <summaries_dir> with your own dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating federalist paper #20\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #58\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #49\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #55\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #54\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #57\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #56\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #51\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #50\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #53\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #52\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #19\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #62\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #63\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "Evaluating federalist paper #18\n",
      "Accuracy at iteration 100: 0.00\n",
      "Accuracy at iteration 200: 0.00\n",
      "Accuracy at iteration 300: 0.00\n",
      "Accuracy at iteration 400: 0.00\n",
      "Accuracy at iteration 500: 0.00\n",
      "Accuracy at iteration 600: 0.00\n",
      "Accuracy at iteration 700: 0.00\n",
      "Accuracy at iteration 800: 0.00\n",
      "Accuracy at iteration 900: 0.00\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluate validation performance\n",
    "# This is classifying batchs from disputed paper #20\n",
    "\n",
    "total_acc = 0\n",
    "for key, value in eval_X.iteritems():\n",
    "    print \"Evaluating federalist paper #%s\" % key\n",
    "    for i in range(1000):\n",
    "        X_batch, y_batch = sample_batch(value,eval_y[key],batch_size)\n",
    "        predictions, acc_val = sess.run([model.predictions, model.accuracy],feed_dict = {model.input: X_batch, model.labels: y_batch, model.keep_prob:1.0})\n",
    "    #    print \"Predictions:\", predictions\n",
    "    #    print \"Suspected Actual: \", y_batch\n",
    "    #    print \"accuracy: %5.3f\" % acc_val\n",
    "        total_acc += acc_val\n",
    "        if i != 0 and i % 100 == 0:\n",
    "            print \"Accuracy at iteration %d: %.2f\" % (i, total_acc/i)\n",
    "print total_acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
