{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn as sk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "# from __future__ import print_function\n",
    "from __future__ import division\n",
    "from os import listdir\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing data loading functions from Classification-LSTM.ipynb\n",
    "With some modifications, since words don't need to be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data(train_data_dir):\n",
    "    y = []\n",
    "    X = []\n",
    "    all_tokens = []\n",
    "    author_to_id = {}\n",
    "    file_names = []\n",
    "    for author_id, author in enumerate(listdir(train_data_dir)):\n",
    "        author_to_id[author] = author_id\n",
    "        author_path = \"%s/%s\" % (train_data_dir, author)\n",
    "        print author, author_id\n",
    "\n",
    "        for file_name in listdir(author_path):\n",
    "            file_names.append(file_name)\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            y.append(author_id)            \n",
    "            with open(full_path, \"r\") as f:\n",
    "                current = canonicalize_words(f.read())\n",
    "                all_tokens += current\n",
    "                X.append(np.array(current))\n",
    "                \n",
    "#     vocab = Vocabulary(all_tokens)\n",
    "    vocab = None\n",
    "    # replace words with ids\n",
    "#     for i, x in enumerate(X):\n",
    "#         # X[i] = np.array(x) # This line can be used to make sure your words are useful \n",
    "#         X[i] = np.array(vocab.words_to_ids(x))\n",
    "\n",
    "    return vocab, X, y, author_to_id, file_names\n",
    "\n",
    "\n",
    "def id_to_author(author_to_id, id):\n",
    "    '''\n",
    "    Takes a dictionary mapping author names to IDs and an ID, and returns\n",
    "    the author mapped to that ID\n",
    "    '''\n",
    "    for author, author_id in author_to_id.iteritems():\n",
    "        if id == author_id:\n",
    "            return author\n",
    "        \n",
    "def load_eval_data(vocab, eval_data_dir):\n",
    "#     eval_X = {}\n",
    "#     eval_y = {}\n",
    "    eval_X = []\n",
    "    eval_y = []\n",
    "    \n",
    "    file_names = []\n",
    "    for author_id, author in enumerate(listdir(eval_data_dir)):\n",
    "        author_path = \"%s/%s\" % (eval_data_dir, author)\n",
    "\n",
    "        for file_name in listdir(author_path):\n",
    "            file_names.append(file_name)\n",
    "            full_path = \"%s/%s\" % (author_path, file_name)\n",
    "            \n",
    "            with open(full_path, \"r\") as f:\n",
    "#                 current = vocab.words_to_ids(canonicalize_words(f.read()))\n",
    "                current = canonicalize_words(f.read())\n",
    "                \n",
    "            expanded_X = np.array(current)\n",
    "            id = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "#             eval_X[id] = np.array([expanded_X])\n",
    "#             eval_y[id] = np.array([author_to_id[author]])\n",
    "            eval_X.append(np.array([expanded_X]))\n",
    "            eval_y.append(np.array([author_to_id[author]]))\n",
    "                \n",
    "    return eval_X, eval_y, file_names\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    word = word.lower()\n",
    "    return canonicalize_digits(word) # try to canonicalize numbers\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def canonicalize_words(words):\n",
    "    current = []\n",
    "    rep_dict = {'\\n':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xa0':' '\n",
    "                ,'\\xc2':' '\n",
    "                ,'\\xc3':' '\n",
    "                ,'\\xa9':' '\n",
    "                ,'\\xef':' '\n",
    "                ,'\\xbb':' '\n",
    "                ,'\\xbf':' '\n",
    "                ,'\\xa6':' '\n",
    "                ,'\\xb9':' '\n",
    "                ,'\\xa3':' '\n",
    "                ,'\\xbd':' '\n",
    "                ,'\\xb4':' '\n",
    "                ,'\\xcb':' '\n",
    "                ,'\\x9a':' '\n",
    "                ,'\\x86':' '\n",
    "                ,'\\xcf':' '\n",
    "                ,'\\x84':' '\n",
    "                ,'\\xce':' '\n",
    "                ,'\\x87':' '\n",
    "                ,'\\xe2':' '\n",
    "                ,'\\x80':' '\n",
    "                ,'\\x94':' '\n",
    "               }\n",
    "    for word in replace_all(words, rep_dict).split(' '):   \n",
    "        if word:\n",
    "            if word[-1] in ('.', ',', '?', ';', '!'):\n",
    "                punk = word[-1]\n",
    "                current.append(punk)\n",
    "                word = word[0:-1]\n",
    "\n",
    "            word = canonicalize_word(word)\n",
    "            current.append(word)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexander_hamilton 0\n",
      "james_madison 1\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './train_final'\n",
    "vocab, X_train, y_train, author_to_id, train_file_names = load_train_data(train_data_dir)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "eval_X, eval_y, eval_file_names = load_eval_data(vocab, \"unknown_data\")\n",
    "test_X, test_y, test_file_names = load_eval_data(vocab, \"test_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data and labels to dataframes for easier handling in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting data and labels to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = [' '.join(text) for text in X_train]\n",
    "y_train = list(y_train)\n",
    "test_y = [val[0] for val in test_y]\n",
    "test_X = [val[0] for val in test_X]\n",
    "test_X = [' '.join(text) for text in test_X]\n",
    "eval_y = [val[0] for val in eval_y]\n",
    "eval_X = [val[0] for val in eval_X]\n",
    "eval_X = [' '.join(text) for text in eval_X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dataframes from data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bow_df(data, labels):\n",
    "    df = pd.DataFrame(data, columns=['body'])\n",
    "    df['label'] = labels\n",
    "    df['author'] = df['label'].apply(lambda aid: id_to_author(author_to_id, aid))\n",
    "    df['author_last'] = df['author'].apply(lambda name: name.split('_')[1])\n",
    "    return df\n",
    "\n",
    "def add_df_data_counts(df):\n",
    "    df['n_sentences'] = df.body.apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "    df['n_words'] = df.body.apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "    df['n_characters'] = df.body.apply(lambda x: len(x))\n",
    "    return df\n",
    "\n",
    "def get_paper_nums(paper_file_names):\n",
    "    '''\n",
    "    Takes a list of federalist paper file names and returns the paper number from each file name in the same order.\n",
    "    '''\n",
    "    return [int(re.findall(\"\\d+\", f_name)[0]) for f_name in paper_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating dataframes\n",
    "train_df = create_bow_df(X_train, y_train)\n",
    "test_df = create_bow_df(test_X, test_y)\n",
    "eval_df = create_bow_df(eval_X, eval_y)\n",
    "\n",
    "## Adding paper numbers\n",
    "train_df['paper_nums'] = get_paper_nums(train_file_names)\n",
    "test_df['paper_nums'] = get_paper_nums(test_file_names)\n",
    "eval_df['paper_nums'] = get_paper_nums(eval_file_names)\n",
    "\n",
    "# restricting eval dataframe to unkown papers\n",
    "unknown_papers = [18,19,20,49,50,51,52,53,54,55,56,57,58,62,63]\n",
    "eval_df = eval_df[eval_df['paper_nums'].isin(unknown_papers)]\n",
    "\n",
    "# combining train and test into a single dataframe\n",
    "train_test_df = train_df.append(test_df)\n",
    "\n",
    "## Adding counts for sentences, words, and characters\n",
    "train_df = add_df_data_counts(train_df)\n",
    "test_df = add_df_data_counts(test_df)\n",
    "eval_df = add_df_data_counts(eval_df)\n",
    "train_test_df = add_df_data_counts(train_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing data counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexander_hamilton</th>\n",
       "      <td>654</td>\n",
       "      <td>23481</td>\n",
       "      <td>127135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>james_madison</th>\n",
       "      <td>568</td>\n",
       "      <td>19327</td>\n",
       "      <td>105476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    n_sentences  n_words  n_characters\n",
       "author                                                \n",
       "alexander_hamilton          654    23481        127135\n",
       "james_madison               568    19327        105476"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('author')[['n_sentences', 'n_words', 'n_characters']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexander_hamilton</th>\n",
       "      <td>1586</td>\n",
       "      <td>58086</td>\n",
       "      <td>314910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>james_madison</th>\n",
       "      <td>530</td>\n",
       "      <td>21043</td>\n",
       "      <td>116343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    n_sentences  n_words  n_characters\n",
       "author                                                \n",
       "alexander_hamilton         1586    58086        314910\n",
       "james_madison               530    21043        116343"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('author')[['n_sentences', 'n_words', 'n_characters']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>james_madison</th>\n",
       "      <td>1013</td>\n",
       "      <td>33201</td>\n",
       "      <td>182512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               n_sentences  n_words  n_characters\n",
       "author                                           \n",
       "james_madison         1013    33201        182512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.groupby('author')[['n_sentences', 'n_words', 'n_characters']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexander_hamilton</th>\n",
       "      <td>2240</td>\n",
       "      <td>81567</td>\n",
       "      <td>442045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>james_madison</th>\n",
       "      <td>1098</td>\n",
       "      <td>40370</td>\n",
       "      <td>221819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    n_sentences  n_words  n_characters\n",
       "author                                                \n",
       "alexander_hamilton         2240    81567        442045\n",
       "james_madison              1098    40370        221819"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df.groupby('author')[['n_sentences', 'n_words', 'n_characters']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_BOW(training_df, stop_words=None, use_tfidf=False):\n",
    "    # Selecting appropriate vectorizor type\n",
    "    if use_tfidf:\n",
    "        vectorizor = sk.feature_extraction.text.TfidfVectorizer\n",
    "    else:\n",
    "        vectorizor = sk.feature_extraction.text.CountVectorizer\n",
    "        \n",
    "    # Creating count vectorizor\n",
    "    cv = vectorizor(analyzer = \"word\", stop_words=stop_words)\n",
    "    \n",
    "    # fitting bag of words model and learning the vocabulary\n",
    "    train_features = cv.fit_transform(training_df.body)\n",
    "    vocab = cv.get_feature_names()\n",
    "    \n",
    "    # training model\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(train_features, training_df.label)\n",
    "    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "    \n",
    "    print('Learned vocab size:', len(vocab))\n",
    "    print('Shape of term-document matrix [n_samples, vocabulary_size]:', train_features.shape)\n",
    "    \n",
    "    return vocab, cv, clf\n",
    "\n",
    "def predict_BOW(prediction_df, cv, clf):\n",
    "    pred_features = cv.transform(prediction_df.body)\n",
    "    predictions = clf.predict(pred_features)\n",
    "    if 'label' in prediction_df.columns:\n",
    "        actual = np.array(prediction_df.label)\n",
    "        print '\\nPredicted and Actual'\n",
    "    else:\n",
    "        actual = None\n",
    "        print '\\Predicted, no Actual for reference'\n",
    "    \n",
    "    correct = sum(predictions == actual)\n",
    "    accuracy = float(correct) / len(predictions)\n",
    "    \n",
    "    # creating dataframe of results with author names\n",
    "    results_df = prediction_df[['paper_nums','author_last']].copy()\n",
    "    results_df.rename(columns={'author_last':'actual'}, inplace=True)\n",
    "    results_df['predictions'] = [id_to_author(author_to_id, pred_id) for pred_id in predictions]\n",
    "    results_df['predictions'] = results_df['predictions'].apply(lambda name: name.split('_')[1])\n",
    "    results_df['is_correcct'] = results_df['actual'] == results_df['predictions']\n",
    "    \n",
    "    print predictions\n",
    "    print actual\n",
    "    print '\\nAccuracy: ', \"%.2f\" % round(accuracy*100,2), '%'\n",
    "    return predictions, actual, correct, accuracy, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Test Papers\n",
    "Training is on all four combinations of None/english stop words and Tfdif/no Tfdif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4457)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4457))\n",
      "\n",
      "Predicted and Actual\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  84.38 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words=None, use_tfidf=False)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(test_df, cv, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4214)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4214))\n",
      "\n",
      "Predicted and Actual\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  75.00 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words='english', use_tfidf=False)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(test_df, cv, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4457)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4457))\n",
      "\n",
      "Predicted and Actual\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  78.13 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words=None, use_tfidf=True)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(test_df, cv, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4214)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4214))\n",
      "\n",
      "Predicted and Actual\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  78.13 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words='english', use_tfidf=True)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(test_df, cv, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Unkown Papers\n",
    "Training is on all four combinations of None/english stop words and Tfdif/no Tfdif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4457)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4457))\n",
      "\n",
      "Predicted and Actual\n",
      "[1 1 1 0 1 1 1 0 0 1 0 0 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  66.67 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words=None, use_tfidf=False)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(eval_df, cv, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4214)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4214))\n",
      "\n",
      "Predicted and Actual\n",
      "[1 1 1 1 1 1 0 0 0 1 0 0 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  66.67 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words='english', use_tfidf=False)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(eval_df, cv, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4457)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4457))\n",
      "\n",
      "Predicted and Actual\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  0.00 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words=None, use_tfidf=True)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(eval_df, cv, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learned vocab size:', 4214)\n",
      "('Shape of term-document matrix [n_samples, vocabulary_size]:', (16, 4214))\n",
      "\n",
      "Predicted and Actual\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Accuracy:  0.00 %\n"
     ]
    }
   ],
   "source": [
    "vocab, cv, clf = train_BOW(train_df, stop_words='english', use_tfidf=True)\n",
    "predictions, actual, correct, accuracy, results_df = predict_BOW(eval_df, cv, clf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
